[["index.html", "Fish eDNA Introduction Objectives Dataset Alberta fishes Methods and their associated available databases Representation of Alberta freshwater fishes in databases", " Fish eDNA Introduction Objectives Dataset The real data set consists of 18 water samples obtained from rivers and lakes near oil-sands extraction sites in the province of Alberta, Canada. Water samples were filtered and DNA collected on the filters was extracted and sequenced. For each sample, two different marker genes were used to amplify and sequence distinct DNA regions : the 12S ribosomal RNA gene and the mitochondrial cytochrome oxidase subunit I (COI). Alberta fishes Fishbase was used to generate the following list of Alberta freshwater fishes. Methods and their associated available databases Within the framework of this project, the following methods and their associated databases were evaluated in regards to their capacities of correctly identifying sequences belonging to freshwater fishes from Alberta. Links [1] Barcode of life (BOLD) [2] barque workflow [3] COI formatted for Barque [4] DADA2 workflow [5] Eukaryote CO1 Classifier [6] Mitochondrial Genome Database of Fish (MitoFish) [7] National Center for Biotechnology Information (NCBI) [8] Ribosomal Database Project (RDP) classifier [9] VSEARCH [10] 12S fish Classifier v1.0.1 [11] 12S formatted for barque Representation of Alberta freshwater fishes in databases Each database was queried to evaluate the presence of reference sequences for Alberta freshwater fishes. Code used to queried the databases can be found here. The absence of the following fishes from any databases can be attributed to different names being used. The presence of these other names in the databases was therefore also evaluated. "],["required-files-scripts-and-programs.html", "Required files, scripts and programs Programs Databases Scripts", " Required files, scripts and programs Programs seqkit v2.1 (website) Installed via Bioconda (link) Base environment needs to be activated in order to use the tool RDP tool classifier v2.11 (Github / SourceForge) To install the RDP classifier I recommend downloading the package from SourceForge as the Github is no longer maintained. Once the download is complete, unzip the folder and locate the file classifer.jar inside the directory dist usearch v11.0.667 (website) To install : (1) Download binaries (2) Unzip and rename the file to usearch (3) Make file executable chmod +x usearch. Place file in an easily accessible directory (for example ~/usearch) vsearch v2.15.2 (Github) Installed via Bioconda (link) Base environment needs to be activated in order to use the tool Databases The following databases can be downloaded with following the hyperlinks. COI formatted for Barque Eukaryote CO1 Classifier for RDP V5.1 12S fish Classifier v1.0.1 12S formatted for barque Scripts In order to train the RDP classifier on any database the following scripts are required and can be found here. lineage2taxTrain.py addFullLineage.py "],["processing-raw-fastq.html", "Processing raw fastq DADA2 barque", " Processing raw fastq The two following FASTQ processing workflow were used in the context of this study : DADA2 barque DADA2 # :::::::::::::::::::::::::::::::::::::::::::: 12S MARKER GENE ::::::::::::::::::::::::::::::::::::::::::::::::: ################################################################################ #### ----------------------------- libraries ------------------------------ #### ################################################################################ library(dada2) library(ShortRead) library(Biostrings) library(reticulate) library(dplyr) library(phyloseq) ################################################################################ #### -------------------------- Prepare files ----------------------------- #### ################################################################################ path = &quot;/home/kvilleneuve/fish_edna/raw/12s&quot; myfiles = list.files(path = path, pattern=&quot;fastq.gz&quot;) fnFs = sort(list.files(path,pattern = &quot;_R1_001.fastq.gz&quot;,full.names=TRUE)) fnRs = sort(list.files(path,pattern = &quot;_R2_001.fastq.gz&quot;,full.names=TRUE)) sample.names = sapply(strsplit(basename(fnFs), &quot;_&quot;), `[`, 1) ################################################################################ #### --------------------------- Plot quality ----------------------------- #### ################################################################################ plotQualityProfile(fnFs.filtN) #read quality drops after about 225bp plotQualityProfile(fnRs.filtN) #read quality drops around 225bp ################################################################################ #### ------------------------ Finding primers ----------------------------- #### ################################################################################ FWD = &quot;CCGGTAAAACTCGTGCCAGC&quot; REV = &quot;CATAGTGGGGTATCTAATCCCAGTTTG&quot; allOrients = function(primer){ require(Biostrings) dna = DNAString(primer) orients = c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), RevComp = reverseComplement(dna)) return(sapply(orients,toString)) } FWD.orients = allOrients(FWD) REV.orients = allOrients(REV) primerHits = function(primer,fn){ nhits = vcountPattern(primer,sread(readFastq(fn)),fixed=FALSE) return(sum(nhits&gt;0)) } rbind(FWD.ForwardReads=sapply(FWD.orients,primerHits,fnFs[[1]]), FWD.ReverseReads=sapply(FWD.orients,primerHits,fnRs[[1]]), REV.ForwardReads=sapply(REV.orients,primerHits,fnFs[[1]]), REV.ReverseReads=sapply(REV.orients,primerHits,fnRs[[1]])) ################################################################################ #### ------------------------ Removing primers ---------------------------- #### ################################################################################ cutadapt = &quot;/home/kvilleneuve/anaconda/envs/cutadapt/bin/cutadapt&quot; system2(cutadapt, args = &#39;--version&#39;) path.cut = file.path(path, &quot;cutadapt&quot;) # create folder cutadapt in directory path_raw if(!dir.exists(path.cut)) dir.create(path.cut) fnFs.cut = file.path(path.cut, basename(fnFs)) fnRs.cut = file.path(path.cut, basename(fnRs)) FWD.RC = dada2:::rc(FWD) REV.RC = dada2:::rc(REV) R1.flags = paste(&quot;-g&quot;, FWD, &quot;-a&quot;, REV.RC) R2.flags = paste(&quot;-G&quot;, REV, &quot;-A&quot;, FWD.RC) #Trim primers with cutadapt for(i in seq_along(fnFs)){ system2(cutadapt, arg = c(R1.flags, R2.flags, &quot;-n&quot;, 2, &quot;-m&quot;, 10, &quot;-o&quot;, fnFs.cut[i],&quot;-p&quot;,fnRs.cut[i],fnFs[i],fnRs[i])) } # Sanity check to confirm primers were removed rbind(FWD.ForwardReads=sapply(FWD.orients,primerHits,fnFs.cut[[1]]), FWD.ReverseReads=sapply(FWD.orients,primerHits,fnRs.cut[[1]]), REV.ForwardReads=sapply(REV.orients,primerHits,fnFs.cut[[1]]), REV.ReverseReads=sapply(REV.orients,primerHits,fnRs.cut[[1]])) ################################################################################ #### ------------------------- Filter and trim ---------------------------- #### ################################################################################ cutFs = sort(list.files(path.cut, pattern =&quot;_R1_001.fastq.gz&quot;, full.names=TRUE)) cutRs = sort(list.files(path.cut, pattern =&quot;_R2_001.fastq.gz&quot;, full.names=TRUE)) filtFs = file.path(path,&quot;filtered&quot;, basename(cutFs)) filtRs = file.path(path,&quot;filtered&quot;, basename(cutRs)) out = filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2,2),truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE) head(out) exists = file.exists(filtFs) # All files have reads errF = learnErrors(filtFs[exists], multithread = TRUE) errR = learnErrors(filtRs[exists], multithread = TRUE) #plotErrors(errF,nominalQ=TRUE) #plotErrors(errR,nominalQ=TRUE) derepFs = derepFastq(filtFs[exists],verbose=TRUE) derepRs = derepFastq(filtRs[exists],verbose=TRUE) dadaFs = dada(filtFs, err = errF, multithread = TRUE, pool = &quot;pseudo&quot;) dadaRs = dada(filtRs, err = errR, multithread = TRUE, pool = &quot;pseudo&quot;) mergers = mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE) names(mergers) = sample.names[exists] seqtab = makeSequenceTable(mergers) table(nchar(getSequences(seqtab))) # Most sequences are 259 bp long #seqtab2 = seqtab[,nchar(colnames(seqtab)) %in% 200:300] seqtab.nochim = removeBimeraDenovo(seqtab, method=&quot;consensus&quot;, multithread=TRUE,verbose=TRUE) sum(seqtab.nochim)/sum(seqtab) # 99.9% of reads remain getN = function(x) sum(getUniques(x)) out_ex = row.names(out)[exists] track = cbind(out,sapply(dadaFs,getN),sapply(dadaRs,getN),sapply(mergers,getN), rowSums(seqtab.nochim)) colnames(track) = c(&quot;input&quot;,&quot;filtered&quot;,&quot;denoisedF&quot;,&quot;denoisedR&quot;,&quot;merged&quot;,&quot;nochim&quot;) ps_base = phyloseq(otu_table(seqtab.nochim,taxa_are_rows = FALSE)) dna = Biostrings::DNAStringSet(taxa_names(ps_base)) names(dna) = taxa_names(ps_base) ps_base = merge_phyloseq(ps_base,dna) ps_sub = prune_taxa(taxa_sums(ps_base) &gt; 2, ps_base) # 2982 taxa remain taxa_names(ps_sub)&lt;-paste0(&quot;ASV&quot;,seq(ntaxa(ps_sub))) ps_sub %&gt;% refseq() %&gt;% Biostrings::writeXStringSet(&quot;/home/kvilleneuve/fish_edna/results/12s_dada2_biostring4annotation.fna&quot;, append = FALSE, compress = FALSE, compression_level = NA, format = &quot;fasta&quot;) saveRDS(ps_sub, file = &quot;/home/kvilleneuve/fish_edna/results/12s_dada2_rdpraw_phylo.rds&quot;) # :::::::::::::::::::::::::::::::::::::::::::: COI MARKER GENE ::::::::::::::::::::::::::::::::::::::::::::::::: path = &quot;/home/kvilleneuve/fish_edna/raw/coi&quot; myfiles = list.files(path = path, pattern=&quot;fastq.gz&quot;) fnFs = sort(list.files(path,pattern = &quot;_R1_001.fastq.gz&quot;,full.names=TRUE)) fnRs = sort(list.files(path,pattern = &quot;_R2_001.fastq.gz&quot;,full.names=TRUE)) ################################################################################ #### ------------------------ Finding primers ----------------------------- #### ################################################################################ FWD = &quot;CGTATTTGGYGCYTGRGCCGGRATAGT&quot; REV = &quot;CARAARCTYATRTTRTTYATTCG&quot; allOrients&lt;-function(primer){ require(Biostrings) dna = DNAString(primer) orients = c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), RevComp = reverseComplement(dna)) return(sapply(orients,toString)) } FWD.orients = allOrients(FWD) REV.orients = allOrients(REV) primerHits = function(primer,fn){ nhits&lt;-vcountPattern(primer,sread(readFastq(fn)),fixed = FALSE) return(sum(nhits&gt;0)) } rbind(FWD.ForwardReads = sapply(FWD.orients,primerHits,fnFs[[1]]), FWD.ReverseReads = sapply(FWD.orients,primerHits,fnRs[[1]]), REV.ForwardReads = sapply(REV.orients,primerHits,fnFs[[1]]), REV.ReverseReads = sapply(REV.orients,primerHits,fnRs[[1]])) ################################################################################ #### ------------------------ Removing primers ---------------------------- #### ################################################################################ cutadapt = &quot;/home/kvilleneuve/anaconda/envs/cutadapt/bin/cutadapt&quot; system2(cutadapt, args = &#39;--version&#39;) path.cut = file.path(path,&quot;cutadapt&quot;) if(!dir.exists(path.cut))dir.create(path.cut) fnFs.cut = file.path(path.cut,basename(fnFs)) fnRs.cut = file.path(path.cut,basename(fnRs)) FWD.RC = dada2:::rc(FWD) REV.RC = dada2:::rc(REV) R1.flags = paste(&quot;-g&quot;,FWD,&quot;-a&quot;,REV.RC) R2.flags = paste(&quot;-G&quot;,REV,&quot;-A&quot;,FWD.RC) for(i in seq_along(fnFs)){ system2(cutadapt, arg = c(R1.flags,R2.flags,&quot;-n&quot;,2,&quot;-m&quot;,10,&quot;-o&quot;,fnFs.cut[i],&quot;-p&quot;,fnRs.cut[i],fnFs[i],fnRs[i])) } rbind(FWD.ForwardReads = sapply(FWD.orients,primerHits,fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients,primerHits,fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients,primerHits,fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients,primerHits,fnRs.cut[[1]]) ) cutFs = sort(list.files(path.cut,pattern=&quot;_R1_001.fastq.gz&quot;,full.names=TRUE)) cutRs = sort(list.files(path.cut,pattern=&quot;_R2_001.fastq.gz&quot;,full.names=TRUE)) get.sample.name = function(fname)strsplit(basename(fname),&quot;_L001&quot;)[[1]][1] sample.names = unname(sapply(cutFs,get.sample.name)) head(sample.names) plotQualityProfile(cutFs[1:4])#most reads very short, but of longer reads quality remains pretty high until 250bp plotQualityProfile(cutRs[1:4])#most reads very short, long read quality drops around 225bp filtFs = file.path(path,&quot;filtered&quot;,basename(cutFs)) filtRs = file.path(path,&quot;filtered&quot;,basename(cutRs)) ################################################################################ #### ------------------------- Filter and trim ---------------------------- #### ################################################################################ out = filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2,2), truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE) head(out) exists = file.exists(filtFs)#all files have reads errF = learnErrors(filtFs[exists],multithread = TRUE) errR = learnErrors(filtRs[exists],multithread = TRUE) plotErrors(errF,nominalQ=TRUE) plotErrors(errR,nominalQ=TRUE) derepFs = derepFastq(filtFs[exists], verbose = TRUE) derepRs = derepFastq(filtRs[exists], verbose = TRUE) dadaFs = dada(derepFs, err = errF, multithread = TRUE, pool = &quot;pseudo&quot;) dadaRs = dada(derepRs, err = errR, multithread = TRUE, pool = &quot;pseudo&quot;) mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE) names(mergers) = sample.names[exists] seqtab = makeSequenceTable(mergers) sort(table(nchar(getSequences(seqtab))))#no clear pattern of sequence length distribution, remove products under 100bp # seqtab2&lt;-seqtab[,nchar(colnames(seqtab)) %in% 100:457] seqtab.nochim = removeBimeraDenovo(seqtab, method = &quot;consensus&quot;, multithread = TRUE, verbose = TRUE) sum(seqtab.nochim)/sum(seqtab) #91% of reads remain getN = function(x) sum(getUniques(x)) out_ex = row.names(out)[exists] track = cbind(out,sapply(dadaFs, getN),sapply(dadaRs,getN), sapply(mergers, getN), rowSums(seqtab.nochim)) colnames(track) = c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoisedF&quot;, &quot;denoisedR&quot;, &quot;merged&quot;, &quot;nochim&quot;) ps_base = phyloseq(otu_table(seqtab.nochim,taxa_are_rows = FALSE)) dna = Biostrings::DNAStringSet(taxa_names(ps_base)) names(dna) = taxa_names(ps_base) ps_base = merge_phyloseq(ps_base, dna) taxa_names(ps_base) = paste0(&quot;ASV&quot;, seq(ntaxa(ps_base))) ps_sub = prune_taxa(taxa_sums(ps_base) &gt; 2, ps_base) # remove singleton and doubletons, 1/3 of taxa removed taxa_names(ps_sub) = paste0(&quot;ASV&quot;, seq(ntaxa(ps_sub))) ps_sub %&gt;% refseq() %&gt;% Biostrings::writeXStringSet(&quot;/home/kvilleneuve/fish_edna/results/coi_dada2_biostring4annotation.fna&quot;, append = FALSE, compress = FALSE, compression_level = NA, format = &quot;fasta&quot;) saveRDS(ps_sub, file = &quot;/home/kvilleneuve/fish_edna/results/coi_dada2_rdpraw_phylo.rds&quot;) barque NB. Barque will only work on GNU Linux or OSX Required dependencies bash 4+ python 3.5+ (you can use miniconda3 to install python) python distutils package R 3+ (ubuntu/mint: sudo apt-get install r-base-core) java (ubuntu/mint: sudo apt-get install default-jre) gnu parallel flash (read merger) v1.2.11+ Add the following to bashrc configuration file export PATH=/home/user/FLASH-1.2.11:$PATH vsearch v2.14.2+ (Barque will not work with older versions of vsearch) Depending on how vsearch is installed either export PATH to location of vsearch or activate base conda environnement Download a copy of the Barque repository git clone https://github.com/enormandeau/barque Get or prepare the database(s) (see Formatting database section below) and deposit the fasta.gz file in the 03_databases folder and give it a name that matches the information of the 02_info/primers.csv file. Link to BOLD database Modify the following parameters : In the 02_info/primers.csv for COI marker : COI_kv,CGTATTTGGYGCYTGRGCCGGRATAGT,CARAARCTYATRTTRTTYATTCG,100,450,bold,0.97,0.9,0.85 for 12S marker 12s200pb,GTCGGTAAAACTCGTGCCAGC,CATAGTGGGGTATCTAATCCCAGTTTG,150,350,12S,0.98,0.9,0.85 In the 02_info/barque_config.sh NCPUS=40 CROP_LENGTH=500 MAX_PRIMER_DIFF=11 Maximum number of differences allowed between primer and sequence (MAX_PRIMER_DIFF=9) Launch Barque Recommended parameters : From the Github directory ./barque 02_info/barque_config.sh Once the pipeline has finished running, all result files are found in the 12_results folder. After a run, it is recommended to make a copy of this folder with some additional info cp -r 12_results results_PROJECT_NAME_DATE_SOME_ADDITIONAL_INFO "],["taxonomic-classification.html", "Taxonomic classification", " Taxonomic classification RDP classifer to assign taxonomy to 12S ASVs from 12S fish classifier https://github.com/terrimporter/12SfishClassifier/tree/v1.0.1 java -Xmx1g -jar ~/rdp_classifier_2.14/dist/classifier.jar classify -t /home/kvilleneuve/fish_edna/database/12Sfishclassifier/mydata_trained/rRNAClassifier.properties -o /home/kvilleneuve/fish_edna/outputs/workflow_output/12s_dada2_rdpraw_annotation.out -q /home/kvilleneuve/fish_edna/outputs/workflow_output/12s_dada2_biostring4annotation.fna RDP classifier with curated database java -Xmx1g -jar ~/rdp_classifier_2.14/dist/classifier.jar classify -t /home/kvilleneuve/fish_edna/database/curated12s_db/12s_training_files/rRNAClassifier.properties -o /home/kvilleneuve/fish_edna/outputs/workflow_output/12s_dada2_rdpcur_annotation.out -q /home/kvilleneuve/fish_edna/outputs/workflow_output/12s_dada2_biostring4annotation.fna Annotate using Vsearch # :::::::::::::::: 12S ::::::::::::::::::: # Curated 12SDB vsearch --usearch_global /home/kvilleneuve/fish_edna/results/12s_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/curated_database/12s_alberta_curated_combined_derep.fasta --blast6out /home/kvilleneuve/fish_edna/results/12s_dada2_vsearch_cur_annotation.out --top_hits_only --notrunclabels --id 0.70 --id 0.70 --maxaccepts 0 --maxrejects 0 # Raw DB vsearch --usearch_global /home/kvilleneuve/fish_edna/results/12s_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/12Sfishclassifier/mydata_training/12S_rdp.fasta --blast6out /home/kvilleneuve/fish_edna/results/12s_dada2_vsearch_raw_annotation.out --dbmatched /home/kvilleneuve/fish_edna/results/12s_dada2_vsearch_raw_match.fasta --alnout /home/kvilleneuve/fish_edna/results/12s_dada2_vsearch_raw_test.out --top_hits_only --notrunclabels --id 0.70 --maxaccepts 0 --maxrejects 0 # :::::::::::::::: COI :::::::::::::::::::: vsearch --usearch_global /home/kvilleneuve/fish_edna/results/coi_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/curated_database/coi/coi_alberta_curated_combined_derep.fasta --blast6out /home/kvilleneuve/fish_edna/results/curated_db/coi_dada2_vsearch_curated.out --top_hits_only --notrunclabels --id 0.70 --maxaccepts 0 --maxrejects 0 vsearch --usearch_global /home/kvilleneuve/fish_edna/results/coi_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/curated_database/coi/alberta_curated_combinedCOI.fasta --blast6out /home/kvilleneuve/fish_edna/results/curated_db/coi_dada2_vsearch_curated.out --top_hits_only --notrunclabels --id 0.70 --maxaccepts 0 --maxrejects 0 # Trying vsearch with the untrained RDP databases vsearch --usearch_global /home/kvilleneuve/fish_edna/results/coi_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/raw_db/RDP_COI.fasta --blast6out /home/kvilleneuve/fish_edna/results/curated_db/coi_dada2_vsearch_rdpraw.out --top_hits_only --notrunclabels --id 0.70 --maxaccepts 0 --maxrejects 0 "],["generating-custom-database-of-the-12s-marker-gene.html", "Generating custom database of the 12S marker gene Evaluate resolution blindspots Prepare files for classifiers Notes on resolution blindspot Assessing hybrids", " Generating custom database of the 12S marker gene Sequences from the RDP databases consist of the complete 12S region while the sequences from the barque databases are trimmed to the amplified region of interest. Therefore, I am keeping the sequences from the RDP databases and adding to it some missing sequences. Steps overview Filter RDP database Extract sequence header Keep only sequence header of fish found in Alberta Curate database using the curated sequence names Add missing sequences Download sequences from Genbank Modify sequence header Combine with curated RDP database Filter RDP database Extract sequence header grep -e &quot;&gt;&quot; 12S_rdp.fasta &gt; header_12s_rdp.txt Keep only sequence header of fish found in Alberta ### ------------------------- Load the libraries -------------------------------------------------- #### library(dplyr) library(stringr) ### ------------------------- Save path for output -------------------------------------------------- #### path = &quot;/home/kvilleneuve/fish_edna/database&quot; ### ------------------------- Load list of Alberta freshwater fish -------------------------------------------------- #### alberta_fish = read.csv(&quot;/home/kvilleneuve/fish_edna/database/fishbase_alberta_with_missing.csv&quot;, header = TRUE, check.names = FALSE) species = gsub(&quot; &quot;, &quot;_&quot;, alberta_fish$Species) # Replace space with underscore ### ------------------------- Load database sequence header -------------------------------------------------- #### # I am intentionally specifying the wrong separator because I don&#39;t want to separate the columns yet rdp12s_raw = read.table(&quot;/home/kvilleneuve/fish_edna/database/header_rdp_12S.txt&quot;, sep = &quot;,&quot;) filtered = rdp12s_raw %&gt;% filter(if_any(1, str_detect, paste0(species, collapse = &#39;|&#39;))) write.table(filtered, file = paste(path, &quot;/alberta_ID_&quot;, names(list_db)[i], &quot;.txt&quot;, sep =&quot;&quot;), quote = FALSE, row.names = FALSE, col.names = FALSE) Curate database using the curated sequence names Use seqkit tool to filter databases in order to keep only sequences of freshwater fishes found in Alberta using list of ID generated for each database. For RDP sed -i &#39;s/\\t/;/g&#39; 12S_rdp.fasta sed -i &#39;s/\\t/;/g&#39; alberta_ID_RDP_12S.txt seqkit grep -nrf alberta_ID_RDP_12S.txt 12S_rdp.fasta -o 12s_01_alberta_curated_RDP_raw.fasta Add missing sequences Download sequences from Genbank Missing sequences were downloaded from NCBI Modify sequence header Sequence header of downloaded sequences were manually modified to include all ranks. Combine with curated RDP database # 12S cat 12s_01_alberta_curated_RDP_raw.fasta 12s_02_missing_sequences.fasta &gt; 12s_03_alberta_curated_withmissing.fasta # Normalize length seqkit seq -w 60 12s_03_alberta_curated_withmissing.fasta &gt; 12s_04_alberta_curated_clean.fasta Evaluate resolution blindspots To evaluate resolution blindspot between two or more taxa the the sequence from the databases were trimmed using the Forward and Reverse primer (rev-comp) with Cutadapt. The sequences were then dereplicated using vsearch derep_fulllength and every sequences removed were evaluated by aligning them with MEGA11. Trim # Forward primer cutadapt -g --action=retain CCGGTAAAACTCGTGCCAGC -o 12s_alberta_curated_combined_trimF.fasta 12s_03_alberta_curated_withmissing.fasta -e 0.2 --untrimmed-output readswithnoforward.txt # Reverse primer cutadapt -a CAAACTGGGATTAGATACCCCACTATG -o 12s_alberta_curated_combined_trimFR.fasta 12s_alberta_curated_combined_trimF.fasta Dereplicate vsearch --derep_fulllength 12s_alberta_curated_combined_trimFR.fasta --output 12s_alberta_curated_combined_trim_derep.fasta See section Notes on resolution blindspot for more details about which sequences were removed. Prepare files for classifiers ::::::::::::::: vsearch Classifier for barque ::::::::::::::::: Copy the curate database to folder 03_database in the barque directory Rename file with shorter name gzip file cp /home/kvilleneuve/fish_edna/database/curated12s_db/12s_04_alberta_curated_clean.fasta /home/kvilleneuve/fish_edna/code/barque_12s/03_databases/12Scurated.fasta gzip 12Scurated.fasta ::::::::::::::: RDP Classifier ::::::::::::::::: Prepare taxonomy file Extract sequence header using grep (2) Replace semi-colon with tab (3) remove &gt; grep -e &quot;&gt;&quot; 12s_04_alberta_curated_clean.fasta &gt; 12s_curated_header.txt sed &#39;s/;/\\t/g&#39; 12s_curated_header.txt &gt; header4rdp.txt sed -i &#39;s/&gt;//g&#39; header4rdp.txt Add header sed -i $&#39;1 i\\\\\\nID\\tcellularOrganisms\\tSuperkingdom\\tKingdom\\tPhylum\\tClass\\tOrder\\tFamily\\tGenus\\tSpecies&#39; header4rdp.txt Prepare sequence file sed &#39;s/;/\\t/g&#39; 12s_04_alberta_curated_clean.fasta &gt; seq4rdp.fasta Train python2 lineage2taxTrain.py header4rdp.txt &gt; ready4train_tax.txt python2 addFullLineage.py header4rdp.txt seq4rdp.fasta &gt; ready4train_seqs.fasta java -Xmx10g -jar ~/rdp_classifier_2.14/dist/classifier.jar train -o 12s_training_files -s ready4train_seqs.fasta -t ready4train_tax.txt Output is a directory specified by the parameter -o which should contain the following files : bergeyTrainingTree.xml genus_wordConditionalProbList.txt logWordPrior.txt wordConditionalProbIndexArr.txt Move into this newly created directory and create the file rRNAClassifier.properties with the following text : # Sample ResourceBundle properties file bergeyTree=bergeyTrainingTree.xml probabilityList=genus_wordConditionalProbList.txt probabilityIndex=wordConditionalProbIndexArr.txt wordPrior=logWordPrior.txt classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.14 Notes on resolution blindspot Cottus Sequence : CACCGCGGTTATACGAGAGGCCCAAGTTGACAAACACCGGCGTAAAGCGTGGTTAAGTTAAAAATCGTACTAAAGCCAAACATCTTCAAGACTGTTATACGTAACCGAAGACAGGAAGTTCAACCACGAAAGTCGCTTTATCTGATCTGAATCCACGAAAGCTAAGGAA Which was exactly similar between the following taxa : NC 068673 […] Cottus cognatus NC 028277 […] Cottus bairdii These were all removed from the database and replaced with a single copy of the trimmed sequence with the following sequence header : NC_028277:NC_068673;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Perciformes;Cottidae;Cottus;Cottus_bairdii:cognatus` CCGGTAAAACTCGTGCCAGCCACCGCGGTTATACGAGAGGCCCAAGTTGACAAACACCG GCGTAAAGCGTGGTTAAGTTAAAAATCGTACTAAAGCCAAACATCTTCAAGACTGTTAT ACGTAACCGAAGACAGGAAGTTCAACCACGAAAGTCGCTTTATCTGATCTGAATCCACG AAAGCTAAGGAACAAACTGGGATTAGATACCCCACTATG Salvelinus Sequence : CACCGCGGTTATACGAGAGGCCCTAGTTGATAACTACCGGCGTAAAGAGTGGTTACGGAAAAATGTTTAATAAAGCCGAACACCCCCTCAGCCGTCATACGCACCTGGGGGCACGAAGACCTACTGCGAAAGCAGCTTTAATTGTACCCGAACCCACGACAGCTACGACA Which was exactly similar between the following taxa : NC 000861 […] Salvelinus alpinus NC 037502 […] Salvelinus malma These were all removed from the database and replaced with a single copy of the trimmed sequence with the following sequence header : NC_000861:NC_037502;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Salvelinus;Salvelinus_alpinus:malma CCGGTAAAACTCGTGCCAGCCACCGCGGTTATACGAGAGGCCCTAGTTGATAACTACCG GCGTAAAGAGTGGTTACGGAAAAATGTTTAATAAAGCCGAACACCCCCTCAGCCGTCAT ACGCACCTGGGGGCACGAAGACCTACTGCGAAAGCAGCTTTAATTGTACCCGAACCCAC GACAGCTACGACACAAACTGGGATTAGATACCCCACTATG Salmo There were 2 copies of the same 12S sequences for Salmo_trutta (NC_024032 and NC_010007), only sequence NC_024032 was kept Assessing hybrids If one of the specie has not been found in Alberta the taxa and it’s sequence were remove. List of remove hybrids : Carassius_auratus_x_Cyprinus_carpio Carassius_auratus_x_Cyprinus_carpio_x_Carassius_cuvieri Cyprinus_carpio_wuyuanensis_x_Carassius_auratus Upon researching if Megalobrama amblycephala have been found in Canada and Alberta I found no evidence of such cases. Therefore all the hybrid sequences were removed. NC 013431 […] Carassius auratus x Megalobrama amblyceph ala triploid hybrid NC 035976 […] Carassius auratus x Megalobrama amblycephala x Carassius cuvieri NC 013430 […] Carassius auratus x Megalobrama amblycephala tetraploid hybrid NC 012980 […] natural gynogenetic Carassius auratus red var. Upon researching if the hybrid has been found in Alberta / Canada I found no evidence of the such. The article associated with this genome (https://doi.org/10.1080/23802359.2019.1574636) states that this hybrid was developed in Tonghua, Jilin Province, Republic of China. I therefore removed the hybrid sequence. NC 042195 […] Salvelinus fontinalis x Salvelinus malma "],["generating-custom-database.html", "Generating custom database Notes on resolution blindspot Assessing hybrids", " Generating custom database To build the curated COI database I downloaded the complete COX1 sequences from NCBI. I then validated that our amplified region of interest was included in the downloaded sequences by using cutadapt, this step was especially important for fishes with only partial COX1 sequences. Notes. The reverse primer used below with CutAdapt corresponds to the RevComp of the actual reverse primer sequence (CARAARCTYATRTTRTTYATTCG). /usr/local/bin/cutadapt --action=mask -g TATTTGGYGCYTGRGCCGGRATAG -o trimF.fasta coi_alberta_curated.fasta --untrimmed-output readswithnoforward.txt -e 0.2 # Reverse primer /usr/local/bin/cutadapt --action=mask -a CGAATRAAYAAYATRAGYTTYTG -o trimFR.fasta trimF.fasta --untrimmed-output readswithnoReverse.txt -e 0.3 To evaluate resolution blindspot between two or more taxa the trim FASTA was dereplicated using vsearch derep_fulllength and every sequences removed were evaluated by aligning them with MEGA11. vsearch --derep_fulllength trimFR.fasta --output coi_trimFR_derep.fasta See section Notes on resolution blindspot for more details about which sequences were removed. ::::::::::::::: vsearch Classifier for barque ::::::::::::::::: Copy the curate database to folder 03_database in the barque directory Rename file with shorter name gzip file cp /home/kvilleneuve/fish_edna/database/curatedcoi_db/coi_alberta_curated.fasta /home/kvilleneuve/fish_edna/code/barque/03_databases/ mv coi_alberta_curated.fasta coicuratedncbi.fasta gzip coicuratedncbi.fasta ::::::::::::::: RDP Classifier ::::::::::::::::: Prepare taxonomy file Extract sequence header using grep (2) Replace semi-colon with tab (3) remove &gt; (4) add header grep -e &quot;&gt;&quot; coi_alberta_curated.fasta &gt; coi_header_4RDP.txt # awk &#39;/^&gt;/{sub(&quot;&gt;&quot;, &quot;&gt;&quot;++i&quot;;&quot;)}1&#39; coi_NCBI_header.txt &gt; coi_header_4RDP.txt #cp coi_NCBI_header.txt coi_header_4RDP.txt sed -i &#39;s/;/\\t/g&#39; coi_header_4RDP.txt sed -i &#39;s/&gt;//g&#39; coi_header_4RDP.txt sed -i $&#39;1 i\\\\\\nID\\tcellularOrganisms\\tSuperkingdom\\tKingdom\\tPhylum\\tClass\\tOrder\\tFamily\\tGenus\\tSpecies&#39; coi_header_4RDP.txt Prepare sequence file The sequence file must be in fasta format and contain a unique identifier without any white space. The accession number makes a good identifier. Anything after the first white space is ignored. The following are acceptable: &gt;DQ248313 ACGATTTTGACCCTTCGGGGTCGATCTCCAACCCTTT &gt;JF735302 k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales CCGAGTTTTCAACTCCCAAACCCCTGTGAACATACCA Replace semi-colon with tab sed &#39;s/;/\\t/g&#39; coi_alberta_curated.fasta &gt; coi_seq_4RDP.fasta #cp coi_alberta_curated_derep_NCBI.fasta coi_seq_4RDP.fasta Train python2 lineage2taxTrain.py coi_header_4RDP.txt &gt; ready4train_taxonomy.txt python2 addFullLineage.py coi_header_4RDP.txt coi_seq_4RDP.fasta &gt; ready4train_seqs.fasta java -Xmx10g -jar ~/rdp_classifier_2.14/dist/classifier.jar train -o COI_training_files -s ready4train_seqs.fasta -t ready4train_taxonomy.txt Output is a directory specified by the parameter -o which should contain the following files : bergeyTrainingTree.xml genus_wordConditionalProbList.txt logWordPrior.txt wordConditionalProbIndexArr.txt Move into this newly created directory and create the file rRNAClassifier.properties with the following text : # Sample ResourceBundle properties file bergeyTree=bergeyTrainingTree.xml probabilityList=genus_wordConditionalProbList.txt probabilityIndex=wordConditionalProbIndexArr.txt wordPrior=logWordPrior.txt classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.14 Notes on resolution blindspot Coregonus Sequence : GTATTTGGTGCCTGAGCCGGAATAGTCGGCACAGCCCTAAGCCTTTTAATCCGAGCGGAACTAAGCCAACCTGGGGCTCTTCTGGGGGATGATCAGATTTATAATGTAATCGTCACGGCCCACGCCTTCGTTATGATTTTCTTTATAGTTATGCCAATTATGATTGGAGGCTTTGGAAACTGATTAATTCCACTTATAATCGGGGCCCCCGACATGGCATTTCCCCGAATGAATAATATGAGCTTTTG Which was exactly similar between the following taxa : JX960903;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Coregonus;Coregonus_zenithicus NC_036393;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Coregonus;Coregonus_artedi Both were removed from the database and replaced with a single copy of the trimmed sequence with the following sequence header : similarseq01;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Coregonus;Coregonus_artedi:zenithicus Assessing hybrids The following hybrids were removed considering on one of the species is not found in Alberta. Carassius_auratus_x_Cyprinus_carpio Carassius_auratus_x_Cyprinus_carpio_x_Carassius_cuvieri Cyprinus_carpio_wuyuanensis_x_Carassius_auratus All Megalobrama amblycephala "],["generating-mock-communities.html", "Generating mock communities", " Generating mock communities Using the article by Morney et al. we identified 16 species we believed to be good candidate to include in the mock communities. Four types of mock communities were designed : 16 species - normalized abundance (richness + / evenness +) 16 species - non-normalized (richness + / evenness -) 8 species - normalized abundance (richness - / evenness +) 8 species - normalized abundance (richness - / evenness -) To generate the low richness communities, 8 species were randomly selected from the 16 species. Required libraries library(dplyr) library(stringr) library(phyloseq) Generate and load required files (list of fishes and databases sequence headers) rich_plus = c(&quot;Salvelinus_fontinalis&quot;,&quot;Oncorhynchus_mykiss&quot;,&quot;Lota_lota&quot;,&quot;Culaea_inconstans&quot;,&quot;Thymallus_arcticus&quot;,&quot;Perca_flavescens&quot;,&quot;Coregonus_clupeaformis&quot;,&quot;Prosopium_williamsoni&quot;,&quot;Salmo_trutta&quot;,&quot;Sander_vitreus&quot;,&quot;Couesius_plumbeus&quot;,&quot;Platygobio_gracilis&quot;,&quot;Coregonus_artedi&quot;,&quot;Catostomus_catostomus&quot;,&quot;Percopsis_omiscomaycus&quot;,&quot;Cottus_cognatus&quot;) # Low richness community # Random subsample to ID which fishes to include in the low richness community rich_low = sample(x=fish_id_bad, size = 8) rich_low #&quot;Salvelinus_fontinalis&quot;,&quot;Oncorhynchus_mykiss&quot;,&quot;Lota_lota&quot;,&quot;Culaea_inconstans&quot;,&quot;Thymallus_arcticus&quot;,&quot;Perca_flavescens&quot;,&quot;Coregonus_clupeaformis&quot;,&quot;Prosopium_williamsoni&quot; :::::::::::::::::::::::::::::::: 12S marker gene :::::::::::::::::::::::::::::::: Extract sequence header name grep -e &quot;&gt;&quot; /home/kvilleneuve/fish_edna/database/curated12s_db/12s_alberta_curated_combined.fasta &gt; /home/kvilleneuve/fish_edna/database/curated12s_db/12s_curated_header.txt Generate read count file for each type of communities Subset sequence header to keep only fish from generated list # Load sequence header from database header12s = read.table(&quot;/home/kvilleneuve/fish_edna/database/curated12s_db/12s_alberta_curated_combined_ready.fasta&quot;) # Set reads per samples (number obtain from the DADA2 workflow) average_reads = 80492 ############################################################ ########## High Richness and high evenness community ########### ############################################################ # Subset sequence header to keep only fish from generate list header12s_richHIGH = header12s %&gt;% filter(if_any(1, str_detect, paste0(rich_plus, collapse = &#39;|&#39;))) # Remove &gt; at the beginning header12s_richHIGH$V1 = gsub(&quot;&gt;&quot;, &quot;&quot;, header12s_richHIGH$V1) # Save list write.table(header12s_richHIGH, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/12s_header_richplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE) # Generate read count file for (richness + / evenness +) mock community. We want the sum of all the ASV in our sample to be similar to the average read count per sample in the regular samples. header12s_richHIGH$count = floor(average_reads/nrow(header12s_richHIGH)) write.table(header12s_richHIGH, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/12s_counts_richplusevenplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;\\t&quot;) ########################################################### ########## Low Richness and high evenness community ########### ########################################################### header12s_richLOW = header12s %&gt;% filter(if_any(1, str_detect, paste0(rich_low, collapse = &#39;|&#39;))) # Remove &gt; at the beginning header12s_richLOW$V1 = gsub(&quot;&gt;&quot;, &quot;&quot;, header12s_richLOW$V1) # Save list write.table(header12s_richLOW, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/12s_header_richlow.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE) # Generate readcount_file for (richness - / evenness +) mock community header12s_richLOW$count = floor(average_reads/nrow(header12s_richLOW)) write.table(header12s_richLOW, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/12s_counts_richlowevenplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;\\t&quot;) ########################################################### ############### Low evenness communities ################# ########################################################### # The low-evenness of both richness types of mock community was manually # generated to have an exponential distribution of reads alternating between # non- and consistently identified fishes. # 12s_counts_richplusevenlow.txt # 12s_counts_richlowevenlow.txt Create FASTA for InSilicoSeq Generate FASTA file with sequences from all 16 fishes used to generate mock community. Because InSilicoSeq only keeps the first 300 nucleotides, the generated amplicon sequence did not initially correspond to our amplified region and no merged reads were generated. I are therefore used cutadapt to remove all the nucleotides before the forward primer while retaining the primer. The same methodology was used for the reverse primer in order to generate a reverse sequence. We then need to seperately generate the forward and reverse reads # Forward primer cutadapt --action=retain -g CCGGTAAAACTCGTGCCAGC -o 12s_seqTRIMF.fasta 12S_allnormline.fasta # Reverse primer cutadapt --action=retain -a CAAACTGGGATTAGATACCCCACTATG -o 12s_seqTRIMR.fasta 12S_allnormline.fasta Use InSilicoSeq Use InSilicoSeq pre-computed error models to generate amplicon reads. Illumina instruments : #!bin/bash ############# High Richness and High Evenness ############# # FORWARD read echo &quot;Generating reads for High Richness and High Evenness&quot; iss generate --genomes 12s_seqTRIMF.fasta --readcount_file 12s_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output 12s-richplusevenplus_L001 --cpus 20 # Delete the reverse (R2) and move R1 to folder fastq rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes 12s_seqTRIMR.fasta --readcount_file 12s_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output 12s-richplusevenplus_L001 --cpus 20 rm *R1* mv *.fastq fastq ############# Low Richness and high evenness ############# echo &quot;Generating reads for Low Richness and High Evenness&quot; # FORWARD read iss generate --genomes 12s_seqTRIMF.fasta --readcount_file 12s_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output 12s-richlowevenplus_L001 --cpus 20 # Delete the reverse (R2) and move R1 to folder fastq rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes 12s_seqTRIMR.fasta --readcount_file 12s_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output 12s-richlowevenplus_L001 --cpus 20 # Delete the forward (R1) and move R2 to folder fastq rm *R1* mv *.fastq fastq ############# High Richness and low evenness ############# echo &quot;Generating reads for High Richness and Low Evenness&quot; # FORWARD read iss generate --genomes 12s_seqTRIMF.fasta --readcount_file 12s_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output 12s-richplusevenlow_L001 --cpus 20 # Delete all the reverse (R2) and move R1 to another folder (fastq) rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes 12s_seqTRIMR.fasta --readcount_file 12s_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output 12s-richplusevenlow_L001 --cpus 20 # Delete all the forward (R1) and move R2 to another folder (fastq) rm *R1* mv *.fastq fastq ############# Low Richness and low evenness ############# echo &quot;Generating reads for Low Richness and Low Evenness&quot; # FORWARD read iss generate --genomes 12s_seqTRIMF.fasta --readcount_file 12s_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output 12s-richlowevenlow_L001 --cpus 20 # Delete all the reverse (R2) and move R1 to another folder (fastq) rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes 12s_seqTRIMR.fasta --readcount_file 12s_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output 12s-richlowevenlow_L001 --cpus 20 # Delete all the forward (R1) and move R2 to another folder (fastq) rm *R1* mv *.fastq fastq rm *.vcf The pattern _001 was then manually added to every fastq generated to match pattern from our other samples. :::::::::::::::::::::::::::::::: COI marker gene :::::::::::::::::::::::::::::::: Extract sequence header name grep -e &quot;&gt;&quot; /home/kvilleneuve/fish_edna/database/curatedcoi_db/coi_alberta_curated_NCBI.fasta &gt; /home/kvilleneuve/fish_edna/database/curatedcoi_db/coi_NCBI_header.txt Generate read count files for each type of communities # Load Sequence header from databases headercoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/curatedcoi_db/coi_NCBI_header.txt&quot;) # Set reads per samples (number obtain from the DADA2 workflow) average_reads = 55424 ############################################################ ########## High Richness and evenness community ########### ############################################################ # High richness community rich_plus = c(fish_id_good, fish_id_bad) # Subset sequence header to keep only fish from generated list headercoi_richHIGH = headercoi %&gt;% filter(if_any(1, str_detect, paste0(rich_plus, collapse = &#39;|&#39;))) headercoi_richHIGH$V1 = gsub(&quot;&gt;&quot;, &quot;&quot;, headercoi_richHIGH$V1) write.table(headercoi_richHIGH, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/coi_header_richplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;;&quot;) # Generate read count file for (richness + / evenness +) mock community. We want the sum of all the ASV in our sample to be similar to the average read count per sample in the regular samples. headercoi_richHIGH$count = floor(average_reads/nrow(headercoi_richHIGH)) write.table(headercoi_richHIGH, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/coi_counts_richplusevenplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;\\t&quot;) ########################################################### ########## Low Richness and evenness community ########### ########################################################### # Low richness community rich_low = c(&quot;Oncorhynchus_mykiss_x_Salmo_salar&quot;,&quot;Notropis_atherinoides&quot;,&quot;Percina_caprodes&quot;,&quot;Coregonus_artedi&quot;,&quot;Sander_vitreus&quot;,&quot;Percopsis_omiscomaycus&quot;,&quot;Culaea_inconstans&quot;,&quot;Lota_lota&quot;) # Subset sequence header to keep only fish from generated list headercoi_richLOW = headercoi %&gt;% filter(if_any(1, str_detect, paste0(rich_low, collapse = &#39;|&#39;))) headercoi_richLOW$V1 = gsub(&quot;&gt;&quot;, &quot;&quot;, headercoi_richLOW$V1) write.table(headercoi_richLOW, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/coi_header_richlow.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE) # Generate read count file for (richness - / evenness +) mock community. headercoi_richLOW$count = floor(average_reads/nrow(headercoi_richLOW)) write.table(headercoi_richLOW, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/coi_counts_richlowevenplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;\\t&quot;) ########################################################### ############### Low evenness communities ################# ########################################################### # The low-evenness of both richness types of mock community was manually # generated to have an exponential distribution of reads alternating between # non- and consistently identified fishes. Create FASTA for InSilicoSeq Generate FASTA file with sequences from all 16 fishes used to generate mock community and make this fasta linear seqkit grep -nrf coi_header_richplus.txt Because InSilicoSeq only keeps the first 300 nucleotides, the generated amplicon sequence did not initially correspond to our amplified region and no merged reads were generated. I therefore used cutadapt to remove all the nucleotides before the forward primer while retaining the primer. For the reverse primer, the actual primer sequence was not consistently present in our sequences. Increasing the -e value did allow the primer to be found but this resulted in sequences less then 300 bp for some fishes. I therefore decided to instead cut the trimF fasta at 300 bp. cutadapt --action=retain -g GTATTTGGYGCYTGRGCCGGRATAGT -o coi_seq_TRIMF.fasta coi_all.fasta -e 0.2 cutadapt --action=retain -a CGAATRAAYAAYATRAGYTTYTG -o coi_seq_TRIMR.fasta coi_all.fasta -e 0.2 #cutadapt --length 350 -o coi_seq_TRIMFR.fasta coi_seq_TRIMF.fasta InSilicoSeq Use InSilicoSeq pre-computed error models to generate amplicon reads. # High Richness and high evenness iss generate --genomes coi_seq_TRIMFR.fasta --readcount_file coi_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenplus_L001 # High Richness and low evenness iss generate --genomes coi_seq_TRIMFR.fasta --readcount_file coi_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenlow_L001 # Low Richness and high evenness iss generate --genomes coi_seq_TRIMFR.fasta --readcount_file coi_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenplus_L001 # Low Richness and low evenness iss generate --genomes coi_seq_TRIMFR.fasta --readcount_file coi_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenlow_L001 The pattern _001 was then manually added to every FASTQ generated to match pattern from our other samples. #!/bin/bash ############# High Richness and High Evenness ############# # FORWARD read echo &quot;Generating reads for High Richness and High Evenness&quot; iss generate --genomes coi_seq_TRIMF.fasta --readcount_file coi_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenplus_L001 --cpus 20 # Delete the reverse (R2) and move R1 to folder fastq rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes coi_seq_TRIMR.fasta --readcount_file coi_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenplus_L001 --cpus 20 rm *R1* mv *.fastq fastq ############# Low Richness and high evenness ############# echo &quot;Generating reads for Low Richness and High Evenness&quot; # FORWARD read iss generate --genomes coi_seq_TRIMF.fasta --readcount_file coi_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenplus_L001 --cpus 20 # Delete the reverse (R2) and move R1 to folder fastq rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes coi_seq_TRIMR.fasta --readcount_file coi_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenplus_L001 --cpus 20 # Delete the forward (R1) and move R2 to folder fastq rm *R1* mv *.fastq fastq ############# High Richness and low evenness ############# echo &quot;Generating reads for High Richness and Low Evenness&quot; # FORWARD read iss generate --genomes coi_seq_TRIMF.fasta --readcount_file coi_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenlow_L001 --cpus 20 # Delete all the reverse (R2) and move R1 to another folder (fastq) rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes coi_seq_TRIMR.fasta --readcount_file coi_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenlow_L001 --cpus 20 # Delete all the forward (R1) and move R2 to another folder (fastq) rm *R1* mv *.fastq fastq ############# Low Richness and low evenness ############# echo &quot;Generating reads for Low Richness and Low Evenness&quot; # FORWARD read iss generate --genomes coi_seq_TRIMF.fasta --readcount_file coi_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenlow_L001 --cpus 20 # Delete all the reverse (R2) and move R1 to another folder (fastq) rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes coi_seq_TRIMR.fasta --readcount_file coi_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenlow_L001 --cpus 20 # Delete all the forward (R1) and move R2 to another folder (fastq) rm *R1* mv *.fastq fastq rm *.vcf for f in *.fastq; do mv “\\(f&quot; &quot;\\)(echo”$f” | sed s/IMG/VACATION/)“; done "],["supplementary-workflow.html", "Supplementary workflow Curating databases for distance matrix", " Supplementary workflow Curating databases for distance matrix Required files : barque COI 3 (barque_coi.fasta) RDP COI V5.1.0 5 (rdp_coi.fasta) RDP Filter out sequence which do not belong to class Actinopteri seqkit grep -r -n -p &#39;.*Actinopteri*&#39; coi_rdp.fasta -o coi_rdp_acti.fasta Extract sequences header and print into txt file grep &quot;&gt;&quot; acti_rdp_coi.fasta acti_rdp_coi.fasta The file with the sequence headers can then be passed to R to generate a list of headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint. #### ------------------------- Load libraries ------------------------------------------------------------------#### library(dplyr) library(tidyr) #### ------------------------- Load databases sequence header -------------------------------------------------- #### rdpcoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt&quot;,sep = &quot;;&quot;) #### ------------------------- Format dataframes -------------------------------------------------------------- #### names(rdpcoi) = c(&quot;ID&quot;, &quot;Cellular_organism&quot;,&quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) rdp_coi_actino = subset(rdpcoi, Class == &quot;Actinopteri&quot;) #### ------------------------- Random sampling --------------------------------------------------------------- #### random_sampling_list = list() i = 0 for (genus in unique(rdp_coi_actino$Genus)){ i = i + 1 sub_df = subset(rdp_coi_actino, Genus == genus) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } random_sampling_list[[i]] = rand_sub_df } #### ------------------------- Generate list of headers to keep ----------------------------------------------- #### df = do.call(&quot;rbind&quot;, random_sampling_list) df = df %&gt;% unite(&quot;sequence_header&quot;, ID:Species, remove = FALSE, sep = &quot;;&quot;) write.table(df$sequence_header, &quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_headers_to_keep.txt&quot;, col.names = FALSE, row.names = FALSE, quote = FALSE) Pass the file generated by the R script to seqkit to create a subset database seqkit grep -nrf rdp_coi_headers_to_keep.txt chord_rdp_coi.fasta -o curated_rdp_coi.fasta Barque Remove sequence of taxa which do not belong to phylum Chordata seqkit grep -r -n -p &#39;.*chordata_*&#39; barque_coi.fasta -o chord_barque_coi.fasta Add unique ID to every sequence header, this is required to randomly sample 10 sequences per genus awk &#39;/^&gt;/{sub(&quot;&gt;&quot;, &quot;&gt;&quot;++i&quot;_&quot;)}1&#39; chord_barque_coi.fasta &gt; chord_barque_coi_ID.fasta Extract sequences header and print into txt file grep &quot;&gt;&quot; chord_barque_coi_ID.fasta | sed -e &quot;s/&gt;//&quot; &gt; barque_coi_ID_header.txt grep &quot;&gt;&quot; rdp_coi.fasta | sed -e &quot;s/&gt;//&quot; &gt; rdp_coi_header.txt The file with the sequence headers can then be passed to R to generate a list of which headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint. #### ------------------------- Load libraries ------------------------------------------------------------------#### library(dplyr) #### ------------------------- Load databases sequence header -------------------------------------------------- #### barquecoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_ID_header.txt&quot;, sep = &quot;_&quot;) rdpcoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt&quot;,sep = &quot;;&quot;) #### ------------------------- Format dataframes for merging --------------------------------------------------- #### # The sequence headers of the COI database used by barque has the following format : Phylum_Genus_Species # Therefore, to keep only sequence belonging to class Actinopteri # I am merging the header from the barque COI database with headers from the RDP COI database to get complete # taxonomic rank barquecoi$Species = paste(barquecoi$V2, barquecoi$V3, sep = &quot;_&quot;) # Create column with specie name barquecoi_unique = barquecoi[!duplicated(barquecoi), ] # Keeping only unique value # Add column names to RDP database names(rdpcoi) = c(&quot;ID&quot;, &quot;Cellular_organism&quot;,&quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) rdpcoi_noID = subset(rdpcoi, select = -c(ID)) # Remove column ID (they are unique values) rdpcoi_unique = rdpcoi_noID[!duplicated(rdpcoi_noID), ] # Keeping only unique value #### ------------------------- Merge dataframes -------------------------------------------------------------- #### comb_df = merge(barquecoi_unique, rdpcoi_unique, by = &quot;Species&quot;, all.y = FALSE ) #### ------------------------- Random sampling --------------------------------------------------------------- #### random_sampling_list = list() i = 0 for (genus in unique(comb_df$Genus.x)){ i = i + 1 sub_df = subset(comb_df, Genus.x == genus) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } random_sampling_list[[i]] = rand_sub_df } df = do.call(&quot;rbind&quot;, random_sampling_list) #### ------------------------- Generate list of headers to keep ----------------------------------------------- #### df$barque_coi_header = paste(df$ID, df$Phylum.x, df$Species, sep = &quot;_&quot;) write.table(df$barque_coi_header, &quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_headers_to_keep.txt&quot;,col.names = FALSE, row.names = FALSE, quote = FALSE) Pass the file generated by the R script to seqkit to create a subset database. seqkit grep -nrf barque_coi_headers_to_keep.txt chord_barque_coi.fasta -o curated_barque_coi.fasta "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
