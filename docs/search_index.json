[["index.html", "Fish eDNA Introduction Objectives Dataset Alberta fishes Methods and their associated available databases Representation of Alberta freshwater fishes in databases Notes on OTUs vs ASVs", " Fish eDNA Introduction Objectives Dataset The dataset consists of 18 water samples obtained from rivers and lakes near oil-sands extraction sites in the province of Alberta, Canada. Water samples were filtered and DNA collected on the filters was extracted and sequenced. For each sample, two different marker genes were used to amplify and sequence distinct DNA regions : the 12S ribosomal RNA gene and the mitochondrial cytochrome oxidase subunit (COI). Alberta fishes Fishbase was used to generate the following list of Alberta freshwater fishes. Methods and their associated available databases Within the framework of this project, the following methods and their associated databases were evaluated in regards to their capacities of correctly identifying sequences belonging to freshwater fishes from Alberta. Links [1] Barcode of life (BOLD) [2] barque workflow [3] COI formatted for Barque [4] DADA2 workflow [5] Eukaryote CO1 Classifier [6] Mitochondrial Genome Database of Fish (MitoFish) [7] National Center for Biotechnology Information (NCBI) [8] Ribosomal Database Project (RDP) classifier [9] VSEARCH [10] 12S fish Classifier v1.0.1 [11] 12S formatted for barque Representation of Alberta freshwater fishes in databases Each database was queried to evaluate the presence of reference sequences for Alberta freshwater fishes. Code used to queried the databases can be found here. The absence of the following fishes from any databases can be attributed to different names being used. The presence of these other names in the databases was therefore also evaluated. Notes on OTUs vs ASVs In early metabarcoding studies, ASVs were always grouped together into Operational Taxonomic Units (OTUs), that were treated as equivalent to species. This grouping process aimed to perform two tasks simultaneously: the removal of intra-specific variation to get species-level taxonomic units, and the removal of any remaining erroneous sequences. As new filtering methods have become available, and our ability to remove erroneous sequences has improved, the latter task has dwindled in relevance and while OTU grouping is still widely used, the error rate of ASVs has decreased to the point that we may be able to analyse the haplotype-level dataset with confidence, allowing the exploration of population-level patterns and processes alongside studying the community level. The choice of using ASVs or OTUs largely depends on your research questions and it may well be that both may be appropriate for different aspects of your research. There are three main points that you should keep in mind if continuing at the ASV level: ASVs are much more sensitive to any errors remaining in your dataset, whereas OTUs will collapse and ignore errors that represent only small variations from real sequences. You may need significantly deeper sampling to adequately capture a sufficiently representative set of ASVs for your study than for OTUs. This will depend a lot on your research question, study community and study taxon. Similarly, you may need significantly deeper sampling to recover representative numbers of reads per ASV per sample in order to generate a realistic picture of ASV distribution across your samples, than for OTUs. Again this will depend on research question, study community and study taxon. Source : Creedy, Vogler &amp; Penlington, Natural History Museum London, Bioinformatic Methods for Biodiversity Metabarcoding. (Page consulted 2024/12/20) "],["required-files-scripts-and-programs.html", "Required files, scripts and programs Programs Databases Scripts", " Required files, scripts and programs Programs seqkit v2.1 (website) Installed via Bioconda (link) Base environment needs to be activated in order to use the tool RDP tool classifier v2.11 (Github / SourceForge) To install the RDP classifier I recommend downloading the package from SourceForge as the Github is no longer maintained. Once the download is complete, unzip the folder and locate the file classifer.jar inside the directory dist usearch v11.0.667 (website) To install : (1) Download binaries (2) Unzip and rename the file to usearch (3) Make file executable chmod +x usearch. Place file in an easily accessible directory (for example ~/usearch) vsearch v2.15.2 (Github) Installed via Bioconda (link) Base environment needs to be activated in order to use the tool Databases The following databases can be downloaded with following the hyperlinks. COI formatted for Barque Eukaryote CO1 Classifier for RDP V5.1 12S fish Classifier v1.0.1 12S formatted for barque Scripts In order to train the RDP classifier on any database the following scripts are required and can be found here. lineage2taxTrain.py addFullLineage.py "],["processing-raw-fastq.html", "Processing raw fastq DADA2 barque", " Processing raw fastq The two following FASTQ processing workflow were used in the context of this study : DADA2 barque DADA2 # :::::::::::::::::::::::::::::::::::::::::::: 12S MARKER GENE ::::::::::::::::::::::::::::::::::::::::::::::::: ################################################################################ #### ----------------------------- libraries ------------------------------ #### ################################################################################ library(dada2) library(ShortRead) library(Biostrings) library(reticulate) library(dplyr) library(phyloseq) ################################################################################ #### -------------------------- Prepare files ----------------------------- #### ################################################################################ path = &quot;/home/kvilleneuve/fish_edna/raw/12s&quot; myfiles = list.files(path = path, pattern=&quot;fastq.gz&quot;) fnFs = sort(list.files(path,pattern = &quot;_R1_001.fastq.gz&quot;,full.names=TRUE)) fnRs = sort(list.files(path,pattern = &quot;_R2_001.fastq.gz&quot;,full.names=TRUE)) sample.names = sapply(strsplit(basename(fnFs), &quot;_&quot;), `[`, 1) ################################################################################ #### --------------------------- Plot quality ----------------------------- #### ################################################################################ plotQualityProfile(fnFs.filtN) #read quality drops after about 225bp plotQualityProfile(fnRs.filtN) #read quality drops around 225bp ################################################################################ #### ------------------------ Finding primers ----------------------------- #### ################################################################################ FWD = &quot;CCGGTAAAACTCGTGCCAGC&quot; REV = &quot;CATAGTGGGGTATCTAATCCCAGTTTG&quot; allOrients = function(primer){ require(Biostrings) dna = DNAString(primer) orients = c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), RevComp = reverseComplement(dna)) return(sapply(orients,toString)) } FWD.orients = allOrients(FWD) REV.orients = allOrients(REV) primerHits = function(primer,fn){ nhits = vcountPattern(primer,sread(readFastq(fn)),fixed=FALSE) return(sum(nhits&gt;0)) } rbind(FWD.ForwardReads=sapply(FWD.orients,primerHits,fnFs[[1]]), FWD.ReverseReads=sapply(FWD.orients,primerHits,fnRs[[1]]), REV.ForwardReads=sapply(REV.orients,primerHits,fnFs[[1]]), REV.ReverseReads=sapply(REV.orients,primerHits,fnRs[[1]])) ################################################################################ #### ------------------------ Removing primers ---------------------------- #### ################################################################################ cutadapt = &quot;/home/kvilleneuve/anaconda/envs/cutadapt/bin/cutadapt&quot; system2(cutadapt, args = &#39;--version&#39;) path.cut = file.path(path, &quot;cutadapt&quot;) # create folder cutadapt in directory path_raw if(!dir.exists(path.cut)) dir.create(path.cut) fnFs.cut = file.path(path.cut, basename(fnFs)) fnRs.cut = file.path(path.cut, basename(fnRs)) FWD.RC = dada2:::rc(FWD) REV.RC = dada2:::rc(REV) R1.flags = paste(&quot;-g&quot;, FWD, &quot;-a&quot;, REV.RC) R2.flags = paste(&quot;-G&quot;, REV, &quot;-A&quot;, FWD.RC) #Trim primers with cutadapt for(i in seq_along(fnFs)){ system2(cutadapt, arg = c(R1.flags, R2.flags, &quot;-n&quot;, 2, &quot;-m&quot;, 10, &quot;-o&quot;, fnFs.cut[i],&quot;-p&quot;,fnRs.cut[i],fnFs[i],fnRs[i])) } # Sanity check to confirm primers were removed rbind(FWD.ForwardReads=sapply(FWD.orients,primerHits,fnFs.cut[[1]]), FWD.ReverseReads=sapply(FWD.orients,primerHits,fnRs.cut[[1]]), REV.ForwardReads=sapply(REV.orients,primerHits,fnFs.cut[[1]]), REV.ReverseReads=sapply(REV.orients,primerHits,fnRs.cut[[1]])) ################################################################################ #### ------------------------- Filter and trim ---------------------------- #### ################################################################################ cutFs = sort(list.files(path.cut, pattern =&quot;_R1_001.fastq.gz&quot;, full.names=TRUE)) cutRs = sort(list.files(path.cut, pattern =&quot;_R2_001.fastq.gz&quot;, full.names=TRUE)) filtFs = file.path(path,&quot;filtered&quot;, basename(cutFs)) filtRs = file.path(path,&quot;filtered&quot;, basename(cutRs)) out = filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2,2),truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE) head(out) exists = file.exists(filtFs) # All files have reads errF = learnErrors(filtFs[exists], multithread = TRUE) errR = learnErrors(filtRs[exists], multithread = TRUE) #plotErrors(errF,nominalQ=TRUE) #plotErrors(errR,nominalQ=TRUE) derepFs = derepFastq(filtFs[exists],verbose=TRUE) derepRs = derepFastq(filtRs[exists],verbose=TRUE) dadaFs = dada(filtFs, err = errF, multithread = TRUE, pool = &quot;pseudo&quot;) dadaRs = dada(filtRs, err = errR, multithread = TRUE, pool = &quot;pseudo&quot;) mergers = mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE) names(mergers) = sample.names[exists] seqtab = makeSequenceTable(mergers) table(nchar(getSequences(seqtab))) # Most sequences are 259 bp long #seqtab2 = seqtab[,nchar(colnames(seqtab)) %in% 200:300] seqtab.nochim = removeBimeraDenovo(seqtab, method=&quot;consensus&quot;, multithread=TRUE,verbose=TRUE) sum(seqtab.nochim)/sum(seqtab) # 99.9% of reads remain getN = function(x) sum(getUniques(x)) out_ex = row.names(out)[exists] track = cbind(out,sapply(dadaFs,getN),sapply(dadaRs,getN),sapply(mergers,getN), rowSums(seqtab.nochim)) colnames(track) = c(&quot;input&quot;,&quot;filtered&quot;,&quot;denoisedF&quot;,&quot;denoisedR&quot;,&quot;merged&quot;,&quot;nochim&quot;) ps_base = phyloseq(otu_table(seqtab.nochim,taxa_are_rows = FALSE)) dna = Biostrings::DNAStringSet(taxa_names(ps_base)) names(dna) = taxa_names(ps_base) ps_base = merge_phyloseq(ps_base,dna) ps_sub = prune_taxa(taxa_sums(ps_base) &gt; 2, ps_base) # 2982 taxa remain taxa_names(ps_sub)&lt;-paste0(&quot;ASV&quot;,seq(ntaxa(ps_sub))) ps_sub %&gt;% refseq() %&gt;% Biostrings::writeXStringSet(&quot;/home/kvilleneuve/fish_edna/results/12s_dada2_biostring4annotation.fna&quot;, append = FALSE, compress = FALSE, compression_level = NA, format = &quot;fasta&quot;) saveRDS(ps_sub, file = &quot;/home/kvilleneuve/fish_edna/results/12s_dada2_rdpraw_phylo.rds&quot;) # :::::::::::::::::::::::::::::::::::::::::::: COI MARKER GENE ::::::::::::::::::::::::::::::::::::::::::::::::: path = &quot;/home/kvilleneuve/fish_edna/raw/coi&quot; myfiles = list.files(path = path, pattern=&quot;fastq.gz&quot;) fnFs = sort(list.files(path,pattern = &quot;_R1_001.fastq.gz&quot;,full.names=TRUE)) fnRs = sort(list.files(path,pattern = &quot;_R2_001.fastq.gz&quot;,full.names=TRUE)) ################################################################################ #### ------------------------ Finding primers ----------------------------- #### ################################################################################ FWD = &quot;CGTATTTGGYGCYTGRGCCGGRATAGT&quot; REV = &quot;CARAARCTYATRTTRTTYATTCG&quot; allOrients&lt;-function(primer){ require(Biostrings) dna = DNAString(primer) orients = c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), RevComp = reverseComplement(dna)) return(sapply(orients,toString)) } FWD.orients = allOrients(FWD) REV.orients = allOrients(REV) primerHits = function(primer,fn){ nhits&lt;-vcountPattern(primer,sread(readFastq(fn)),fixed = FALSE) return(sum(nhits&gt;0)) } rbind(FWD.ForwardReads = sapply(FWD.orients,primerHits,fnFs[[1]]), FWD.ReverseReads = sapply(FWD.orients,primerHits,fnRs[[1]]), REV.ForwardReads = sapply(REV.orients,primerHits,fnFs[[1]]), REV.ReverseReads = sapply(REV.orients,primerHits,fnRs[[1]])) ################################################################################ #### ------------------------ Removing primers ---------------------------- #### ################################################################################ cutadapt = &quot;/home/kvilleneuve/anaconda/envs/cutadapt/bin/cutadapt&quot; system2(cutadapt, args = &#39;--version&#39;) path.cut = file.path(path,&quot;cutadapt&quot;) if(!dir.exists(path.cut))dir.create(path.cut) fnFs.cut = file.path(path.cut,basename(fnFs)) fnRs.cut = file.path(path.cut,basename(fnRs)) FWD.RC = dada2:::rc(FWD) REV.RC = dada2:::rc(REV) R1.flags = paste(&quot;-g&quot;,FWD,&quot;-a&quot;,REV.RC) R2.flags = paste(&quot;-G&quot;,REV,&quot;-A&quot;,FWD.RC) for(i in seq_along(fnFs)){ system2(cutadapt, arg = c(R1.flags,R2.flags,&quot;-n&quot;,2,&quot;-m&quot;,10,&quot;-o&quot;,fnFs.cut[i],&quot;-p&quot;,fnRs.cut[i],fnFs[i],fnRs[i])) } rbind(FWD.ForwardReads = sapply(FWD.orients,primerHits,fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients,primerHits,fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients,primerHits,fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients,primerHits,fnRs.cut[[1]]) ) cutFs = sort(list.files(path.cut,pattern=&quot;_R1_001.fastq.gz&quot;,full.names=TRUE)) cutRs = sort(list.files(path.cut,pattern=&quot;_R2_001.fastq.gz&quot;,full.names=TRUE)) get.sample.name = function(fname)strsplit(basename(fname),&quot;_L001&quot;)[[1]][1] sample.names = unname(sapply(cutFs,get.sample.name)) head(sample.names) plotQualityProfile(cutFs[1:4])#most reads very short, but of longer reads quality remains pretty high until 250bp plotQualityProfile(cutRs[1:4])#most reads very short, long read quality drops around 225bp filtFs = file.path(path,&quot;filtered&quot;,basename(cutFs)) filtRs = file.path(path,&quot;filtered&quot;,basename(cutRs)) ################################################################################ #### ------------------------- Filter and trim ---------------------------- #### ################################################################################ out = filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2,2), truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE) head(out) exists = file.exists(filtFs)#all files have reads errF = learnErrors(filtFs[exists],multithread = TRUE) errR = learnErrors(filtRs[exists],multithread = TRUE) plotErrors(errF,nominalQ=TRUE) plotErrors(errR,nominalQ=TRUE) derepFs = derepFastq(filtFs[exists], verbose = TRUE) derepRs = derepFastq(filtRs[exists], verbose = TRUE) dadaFs = dada(derepFs, err = errF, multithread = TRUE, pool = &quot;pseudo&quot;) dadaRs = dada(derepRs, err = errR, multithread = TRUE, pool = &quot;pseudo&quot;) mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE) names(mergers) = sample.names[exists] seqtab = makeSequenceTable(mergers) sort(table(nchar(getSequences(seqtab))))#no clear pattern of sequence length distribution, remove products under 100bp # seqtab2&lt;-seqtab[,nchar(colnames(seqtab)) %in% 100:457] seqtab.nochim = removeBimeraDenovo(seqtab, method = &quot;consensus&quot;, multithread = TRUE, verbose = TRUE) sum(seqtab.nochim)/sum(seqtab) #91% of reads remain getN = function(x) sum(getUniques(x)) out_ex = row.names(out)[exists] track = cbind(out,sapply(dadaFs, getN),sapply(dadaRs,getN), sapply(mergers, getN), rowSums(seqtab.nochim)) colnames(track) = c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoisedF&quot;, &quot;denoisedR&quot;, &quot;merged&quot;, &quot;nochim&quot;) ps_base = phyloseq(otu_table(seqtab.nochim,taxa_are_rows = FALSE)) dna = Biostrings::DNAStringSet(taxa_names(ps_base)) names(dna) = taxa_names(ps_base) ps_base = merge_phyloseq(ps_base, dna) taxa_names(ps_base) = paste0(&quot;ASV&quot;, seq(ntaxa(ps_base))) ps_sub = prune_taxa(taxa_sums(ps_base) &gt; 2, ps_base) # remove singleton and doubletons, 1/3 of taxa removed taxa_names(ps_sub) = paste0(&quot;ASV&quot;, seq(ntaxa(ps_sub))) ps_sub %&gt;% refseq() %&gt;% Biostrings::writeXStringSet(&quot;/home/kvilleneuve/fish_edna/results/coi_dada2_biostring4annotation.fna&quot;, append = FALSE, compress = FALSE, compression_level = NA, format = &quot;fasta&quot;) saveRDS(ps_sub, file = &quot;/home/kvilleneuve/fish_edna/results/coi_dada2_rdpraw_phylo.rds&quot;) barque NB. Barque will only work on GNU Linux or OSX Required dependencies bash 4+ python 3.5+ (you can use miniconda3 to install python) python distutils package R 3+ (ubuntu/mint: sudo apt-get install r-base-core) java (ubuntu/mint: sudo apt-get install default-jre) gnu parallel flash (read merger) v1.2.11+ Add the following to bashrc configuration file export PATH=/home/user/FLASH-1.2.11:$PATH vsearch v2.14.2+ (Barque will not work with older versions of vsearch) Depending on how vsearch is installed either export PATH to location of vsearch or activate base conda environnement Download a copy of the Barque repository git clone https://github.com/enormandeau/barque Get or prepare the database(s) (see Formatting database section below) and deposit the fasta.gz file in the 03_databases folder and give it a name that matches the information of the 02_info/primers.csv file. Link to BOLD database Modify the following parameters : In the 02_info/primers.csv for COI marker : COI_kv,CGTATTTGGYGCYTGRGCCGGRATAGT,CARAARCTYATRTTRTTYATTCG,100,450,bold,0.97,0.9,0.85 for 12S marker 12s200pb,GTCGGTAAAACTCGTGCCAGC,CATAGTGGGGTATCTAATCCCAGTTTG,150,350,12S,0.98,0.9,0.85 In the 02_info/barque_config.sh NCPUS=40 CROP_LENGTH=500 MAX_PRIMER_DIFF=11 Maximum number of differences allowed between primer and sequence (MAX_PRIMER_DIFF=9) Launch Barque Recommended parameters : From the Github directory ./barque 02_info/barque_config.sh Once the pipeline has finished running, all result files are found in the 12_results folder. After a run, it is recommended to make a copy of this folder with some additional info cp -r 12_results results_PROJECT_NAME_DATE_SOME_ADDITIONAL_INFO "],["taxonomic-classification.html", "Taxonomic classification", " Taxonomic classification RDP classifer to assign taxonomy to 12S ASVs from 12S fish classifier https://github.com/terrimporter/12SfishClassifier/tree/v1.0.1 java -Xmx1g -jar ~/rdp_classifier_2.14/dist/classifier.jar classify -t /home/kvilleneuve/fish_edna/database/12Sfishclassifier/mydata_trained/rRNAClassifier.properties -o /home/kvilleneuve/fish_edna/outputs/workflow_output/12s_dada2_rdpraw_annotation.out -q /home/kvilleneuve/fish_edna/outputs/workflow_output/12s_dada2_biostring4annotation.fna RDP classifier with curated database java -Xmx1g -jar ~/rdp_classifier_2.14/dist/classifier.jar classify -t /home/kvilleneuve/fish_edna/database/curated12s_db/12s_training_files/rRNAClassifier.properties -o /home/kvilleneuve/fish_edna/outputs/workflow_output/12s_dada2_rdpcur_annotation.out -q /home/kvilleneuve/fish_edna/outputs/workflow_output/12s_dada2_biostring4annotation.fna Annotate using Vsearch # :::::::::::::::: 12S ::::::::::::::::::: # Curated 12SDB vsearch --usearch_global /home/kvilleneuve/fish_edna/results/12s_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/curated_database/12s_alberta_curated_combined_derep.fasta --blast6out /home/kvilleneuve/fish_edna/results/12s_dada2_vsearch_cur_annotation.out --top_hits_only --notrunclabels --id 0.70 --id 0.70 --maxaccepts 0 --maxrejects 0 # Raw DB vsearch --usearch_global /home/kvilleneuve/fish_edna/results/12s_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/12Sfishclassifier/mydata_training/12S_rdp.fasta --blast6out /home/kvilleneuve/fish_edna/results/12s_dada2_vsearch_raw_annotation.out --dbmatched /home/kvilleneuve/fish_edna/results/12s_dada2_vsearch_raw_match.fasta --alnout /home/kvilleneuve/fish_edna/results/12s_dada2_vsearch_raw_test.out --top_hits_only --notrunclabels --id 0.70 --maxaccepts 0 --maxrejects 0 # :::::::::::::::: COI :::::::::::::::::::: vsearch --usearch_global /home/kvilleneuve/fish_edna/results/coi_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/curated_database/coi/coi_alberta_curated_combined_derep.fasta --blast6out /home/kvilleneuve/fish_edna/results/curated_db/coi_dada2_vsearch_curated.out --top_hits_only --notrunclabels --id 0.70 --maxaccepts 0 --maxrejects 0 vsearch --usearch_global /home/kvilleneuve/fish_edna/results/coi_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/curated_database/coi/alberta_curated_combinedCOI.fasta --blast6out /home/kvilleneuve/fish_edna/results/curated_db/coi_dada2_vsearch_curated.out --top_hits_only --notrunclabels --id 0.70 --maxaccepts 0 --maxrejects 0 # Trying vsearch with the untrained RDP databases vsearch --usearch_global /home/kvilleneuve/fish_edna/results/coi_dada2_biostring4annotation.fna -db /home/kvilleneuve/fish_edna/database/raw_db/RDP_COI.fasta --blast6out /home/kvilleneuve/fish_edna/results/curated_db/coi_dada2_vsearch_rdpraw.out --top_hits_only --notrunclabels --id 0.70 --maxaccepts 0 --maxrejects 0 "],["evaluate-performance-of-databases.html", "Evaluate performance of databases Description Methods Implementing CVI with our data Generate benchmark datasets", " Evaluate performance of databases Link to article and TAXXI website. Description Edgar (2018) developed a Cross-Validation by Identity (CVI) framework that tests 16 unique classifiers and 3 parameter settings of the SINTAX, RDP, and Non-Bayensien-Classifier classifiers to assign taxonomy to specially modified test and training data sets (Warcup ITS, 16S full length, 16S V4 and 16S V3-5). These data sets were designed by Edgar (2018) to 1) have even representation across genera, and 2) test classifier effectiveness across different loci of the same gene (16S). Each classifier is then assessed for the true positive rate (TPR), the over-classification rate (OCR), the under classification rate (UCR), the misclassification rate (MCR), and the accuracy (ACC). The true positive rate (TPR) indicates how frequently the correct taxonomy was assigned out of the total number of opportunities for correct classification. The over-classification rate (OCR) indicates how frequently too many ranks are predicted for query sequences out of the total opportunities to make an over classification error. The under-classification rate (UCR) indicates how frequently too few ranks are predicted for query sequences out of the total number of opportunities to make this error. The misclassification rate (MCR) indicates how frequently a sequence matching a query is available in the database but is not predicted for that query out to the number of opportunities to make this error. The accuracy (ACC) indicates the number of correct taxonomic calls out of the number of opportunities to determine correct taxonomy. Methods A reference with known taxonomies is split into test and training sets such that for all test sequences, the most similar training sequence has a given identity (d). This is repeated for different identities, enabling assessment of prediction accuracy at varying distances from the reference. - R is the reference dataset divided into four disjoint subsets S, T, W and Z. - S is the test set. - A is the training set formed by the union of T and W. - T is the set of top hits for sequences in S, which are constrained to have identities in the range d ± σ (where σ specifies the maximum allowed deviation from the desired identity (d)). - W contains reference sequences with identity &lt; d; these are retained to create the largest possible training set. - Z contains sequences which cannot be assigned to S, T or W without violating the identity constraint. Implementing CVI with our data TAXXI framework was used to compare the performance of the RDP and vsearch classifiers with the following databases : COI BOLD 3 COI Eukaryote V5.1.0 5 12S MitoFISH V1.0.1 Custom 12S from barque Considering the shear size of both COI databases and the recommendation by Edgard to limit the number of sequences per species, an R script was used to randomly select 10 sequences per species for these databases. Workflow used for this can be found here. Generate benchmark datasets The distmx_split_identity command from USEARCH divides sequences into subsets such that the top-hit identity is a given value. This is used to create test-training pairs for cross-validation by identity. Input is a distance matrix created by the calc_distmx command. As per the methods specified in Edgar (2018) maximum allowed deviation (σ) from d used : σ = 1% for d = 90% and σ = 0.5% for d = 99, 97 and 95%. The following code was adapted from Donhauser et al. (2024) and validated with available documentation from USEARCH. "],["taxonomy-cross-validation-by-identity-taxxi.html", "Taxonomy cross validation by identity (TAXXI) Description Methods Implementing CVI with our data Generate benchmark datasets Generate predictions CVI metrics", " Taxonomy cross validation by identity (TAXXI) Link to article and TAXXI website. Description Edgar (2018) developed a Cross-Validation by Identity (CVI) framework that tests 16 unique classifiers and 3 parameter settings of the SINTAX, RDP, and Non-Bayensien-Classifier classifiers to assign taxonomy to specially modified test and training data sets (Warcup ITS, 16S full length, 16S V4 and 16S V3-5). These data sets were designed by Edgar (2018) to 1) have even representation across genera, and 2) test classifier effectiveness across different loci of the same gene (16S). Each classifier is then assessed for the true positive rate (TPR), the over-classification rate (OCR), the under classification rate (UCR), the misclassification rate (MCR), and the accuracy (ACC). The true positive rate (TPR) indicates how frequently the correct taxonomy was assigned out of the total number of opportunities for correct classification. The over-classification rate (OCR) indicates how frequently too many ranks are predicted for query sequences out of the total opportunities to make an over classification error. The under-classification rate (UCR) indicates how frequently too few ranks are predicted for query sequences out of the total number of opportunities to make this error. The misclassification rate (MCR) indicates how frequently a sequence matching a query is available in the database but is not predicted for that query out to the number of opportunities to make this error. The accuracy (ACC) indicates the number of correct taxonomic calls out of the number of opportunities to determine correct taxonomy. Methods A reference with known taxonomies is split into test and training sets such that for all test sequences, the most similar training sequence has a given identity (d). This is repeated for different identities, enabling assessment of prediction accuracy at varying distances from the reference. - R is the reference dataset divided into four disjoint subsets S, T, W and Z. - S is the test set. - A is the training set formed by the union of T and W. - T is the set of top hits for sequences in S, which are constrained to have identities in the range d ± σ (where σ specifies the maximum allowed deviation from the desired identity (d)). - W contains reference sequences with identity &lt; d; these are retained to create the largest possible training set. - Z contains sequences which cannot be assigned to S, T or W without violating the identity constraint. Implementing CVI with our data TAXXI framework was used to compare the performance of the RDP and vsearch classifiers with the following databases : barque COI 3 RDP COI V5.1.0 5 Considering the size of the original COI databases, the memory limit of 32-bit process was exceeded, and therefore the 64-bit build was required. To overcome this issue both databases were pre-curated and sub-sampled. Workflow used for this can be found here. Because the 12S databases did not include multiple sequences per species the TAXXI framework could not be used to evaluate these databases. Generate benchmark datasets The distmx_split_identity command from USEARCH divides sequences into subsets such that the top-hit identity is a given value. This is used to create test-training pairs for cross-validation by identity. Input is a distance matrix created by the calc_distmx command. As per the methods specified in Edgar (2018) maximum allowed deviation (σ) from d used : σ = 1% for d = 90% and σ = 0.5% for d = 99, 97 and 95%. The following code was adapted from Donhauser et al. (2024) and validated with available documentation from USEARCH. 1. Format databases Remove tab spaces in sequence name (for RDP databases specifically) for i in *.fasta ; do sed -i &#39;s/\\t/;/g&#39; $i ; done All the sequences from the COI database used with RDP were lowercase. The following code was used to make them uppercase seqkit seq mytrainseq.fasta --upper-case -w 0 &gt; rdp_coi.fasta 2. Generate training and test sets Create a distance matrix using function -calc_distmx for i in *.fasta ; do ~/usearch -calc_distmx $i -maxdist 0.2 -termdist 0.3 -tabbedout ${i%%.*}_distmax.txt; done /home/karine/usearch11 -calc_distmx coi_vsearch_10sp.fasta -maxdist 0.2 -termdist 0.3 -tabbedout coi_vsearch_distmax.txt To create training and test sets at different identity thresholds for i in *.fasta ; do ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.025 -maxdist 0.035 -tabbedout ${i%%.*}.97.subsets.txt ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.045 -maxdist 0.055 -tabbedout ${i%%.*}.95.subsets.txt ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.095 -maxdist 0.105 -tabbedout ${i%%.*}.90.subsets.txt done Output is a tabbed text given by the -tabbedout option. Fields are: Col1 - Subset name : there are four subsets with names 1, 2, 1x and 2x. Col2 - Label1 : label1 is the label of a sequence in the subset given by Col1. Col3 - Label2 : label2 is the top hit in the other subset (1 or 2) Col4 - Dist : distance between Label1 and Label2. Subsets 1 and 2 have top hits to each other in the specified range. Subset 1x has lower identities with subset 2, and can therefore be added to the training set if subset 2 is the query set. Similarly, subset 2x has lower identities with subset 1 and can be added to the training set if subset 1 is the query Get sequence ID for training and test set for i in *.subsets.txt ; do awk &#39;$1==1 || $1==&quot;1x&quot; {print $2}&#39; $i &gt; $i.trainingIDs.txt; awk &#39;$1==2 {print $2}&#39; $i &gt; $i.testIDs.txt; done Create fasta file with test and training set at each identity, use subset 1 as training and subset 2 as test. If sekqit is installed through conda activate base environment first. for i in *.trainingIDs.txt ; do seqkit grep -n -f $i ${i%%.*}.fasta &gt; $i.trainning.fasta ; done for i in *.testIDs.txt ; do seqkit grep -n -f $i ${i%%.*}.fasta &gt; $i.test.fasta ; done 3. Rename files for file in *subsets* ; do mv $file ${file//.9/_9} ; done Generate predictions RDP Classifier Following steps were adapted from John Quensen’s tutorial (link) and available scripts associated with publication by Miranda (2020). Required files To train the newly generated training set for each identity level you need to generate a sequence file and a taxonomy file, each with special formatting requirements : Sequence file The sequence file must be in FASTA format and contain a unique identifier without any white space. The accession number makes a good identifier. Anything after the first white space is ignored. The following are acceptable: &gt;DQ248313 ACGATTTTGACCCTTCGGGGTCGATCTCCAACCCTTCGGGGTCGATCGATTTTGACCCT &gt;JF735302 k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Dactylonectria;s__Dactylonectria_anthuriicol CCGAGTTTTCAACTCGACCCTTCGGGGTCGTCGATCTCCAACCCGATCGATTTTGAACC Taxonomy file The taxonomy file is a tab-delimited text file beginning with a header giving the Sequence ID and names of ranks to be included. There are two requirements for this file: There must be an entry for every rank in every line. Hyphen placeholders are allowed but are not recommended. “Convergent evolution” is not allowed. For example, the same genus cannot appear in two different families. Options for missing ranks : If a rank does not exist, you can fill in the missing entries with hyphens but it is not recommended as it can lead to a “ragged” classification that cannot be properly sorted by rank. Another option is to fill in the empty spaces with made-up but meaningful ranks as in the table below. The prefixes indicate the highest rank available. The absence of hyphen placeholders means that classification will not be ragged but include all ranks. Thus it will be possible to select, sort, and merge ranks when analyzing your data later. Example format : Seq_ID Kingdom Phylum Class Order Family Genus Species MG190602 Fungi Ascomycota Sordariomycetes Hypocreales o_Hypocreales o_f_Hypocreales MF120484 Fungi Ascomycota Sordariomycetes Hypocreales Nectriaceae Fusarium Scripts lineage2taxTrain.py addFullLineage.py Generate required files Taxonomy file Extract sequence header using grep for i in *.trainning.fasta ; do grep -e &quot;&gt;&quot; $i &gt; ${i%%.*}_trainheader.txt ; done For RDP databases which have the following sequence header format we can simply replace semi-colon with tabs &gt;KJ592636 cellularOrganisms;Eukaryota;undef_Eukaryota;Rhodophyta;Florideophyceae;Hapalidiales;Mesophyllumaceae;Mesophyllum;Mesophyllum_lichenoides For COI databases which have the following format, and Phylum being exclusively chordata we will replace the string _chordata_ with a tab &gt;11_chordata_Abalistes_stellaris for i in *.trainingIDs.txt ; do sed &#39;s/;/\\t/g&#39; $i &gt; ${i%%.*}.RDP_trainID.txt ; done for i in *.trainingIDs.txt ; do sed &#39;s/_chordata_/\\t/g&#39; $i &gt; ${i%%.*}.RDP_trainID.txt ; done Remove “&gt;” for i in *.RDP_trainID.txt ; do sed -i &#39;s/&gt;//g&#39; $i ; done Add header (rank followed by column number i.e. rank_1, rank_2, etc.) for i in *.RDP_trainID.txt ; do awk -i inplace &#39;BEGIN {OFS=FS=&quot;\\t&quot;} NR==1{for (i=1;i&lt;=NF;i++) printf &quot;%s%s&quot;, &quot;rank_&quot;i, i==NF?ORS:OFS}1&#39; $i ; done Details : BEGIN {OFS=FS=“} sets the input and output delimiter to tab, instead of the default space. Change” to your delimiter if its not tab. NR==1{} says to execute the actions only on the first line Sequence file For RDP databases which have the following sequence header format we can simply replace semi-colon with tabs For COI databases which have the following format, and Phylum being exclusively chordata we will replace the string _chordata_ with tabs for i in *.trainning.fasta ; do sed -i &#39;s/;/\\t/g&#39; $i ; done for i in *.trainning.fasta ; do sed -i &#39;s/_chordata_/\\t/g&#39; $i ; done for i in *.RDP_trainID.txt ; do python2 lineage2taxTrain.py $i &gt; ${i%%.*}.ready4train_tax.txt ; done for i in *.trainning.fasta ; do python2 addFullLineage.py ${i%%.*}.RDP_trainID.txt $i &gt; ${i%%.*}.ready4train_seqs.fasta ; done Train the set for i in *.ready4train_tax.txt ; do java -Xmx10g -jar ~/rdp_classifier_2.14/dist/classifier.jar train -o ${i%%.*}_training_files -s ${i%%.*}.ready4train_seqs.fasta -t $i ; done Output is a directory specified by the parameter -o which should contain the following files : bergeyTrainingTree.xml genus_wordConditionalProbList.txt logWordPrior.txt wordConditionalProbIndexArr.txt Move into this newly created directory and create the file rRNAClassifier.properties with the following text : # Sample ResourceBundle properties file bergeyTree=bergeyTrainingTree.xml probabilityList=genus_wordConditionalProbList.txt probabilityIndex=wordConditionalProbIndexArr.txt wordPrior=logWordPrior.txt classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.14 Generate predictions for i in *.ready4train_seqs.fasta ; do java -Xmx1g -jar ~/rdp_classifier_2.14/dist/classifier.jar -q ${i%%.*}.subsets.txt.testIDs.txt.test.fasta -t ./${i%%.*}_training_files/rRNAClassifier.properties -o ${i%%.*}.RDP_predictions.tsv ; done VSEARCH Using option usearch_global (option used by barque) : for i in *.subsets.txt.testIDs.txt.test.fasta ; do vsearch --usearch_global $i -db ${i%%.*}.subsets.txt.trainingIDs.txt.trainning.fasta --blast6out ${i%%.*}.vsearch_predictions.tsv --top_hits_only --notrunclabels --id 0.70; done Output is a tabbed text given by the -blast6out option. Fields are: Col1 : query_name Col2 : target - database sequence label Col3 : id - percentage of identity (real value ranging from 0.0 to 100.0). The per- centage identity is defined as 100 * (matching columns) / (alignment length - terminal gaps). See fields id0 to id4 for other definitions. Col4 : alnlen Col5 : mism Col6 : opens Col7 : qlo Col8 : qhi Col9 : tlo Col10 : thi Col11 : evalue Col12 : bits CVI metrics Important definitions : N : number of sequences in the test set S, K : number of sequences in S with known names (names which are present in the training set A) L : number of novel test sequences (= N – K) (sequences in S with names that are not present in A) TP : number of names which are correctly predicted MC : number of misclassification errors OC : number of over-classification errors UC : number under-classification errors The rate for each type of error is defined as the number of errors divided by the number of opportunities to make that error: OCR = OC/L (over-classification rate), UCR = UC/K (under-classification rate) MCR = MC/K (misclassification rate) TPR = TP/K Acc = TP/(K + OC) For each rank the mean values of the metrics over all test/training pairs for all values of the top-hit identity (d) was calculated and is designated by prefix Avg. True-positive rate (AvgTPR) Under-classification errors (AvgUCR) Misclassification rate (AvgMCR) Over-classification rate (AvgOCR) Average L10Acc Average accuracy (AvgAcc) The lowest common rank (LCR) of two sequences is the lowest rank where both have the same taxon name. The most probable lowest common rank (MLR) for a pair of sequences with identity d is defined as the LCR with highest probability. MLRs can be summarized by giving the rank identity threshold (RIT) for each rank r. The rank identity threshold (RIT) for each rank r is defined as the minimum identity for which MLR(d) = r. For example, if MLR(100) = species, MLR(99) = genus, MLR(98) = genus, … MLR(94) = genus and MLR(93) = family, then RIT(species) = 100 and RIT(genus) = 94. The top-hit identity distribution (THID) is the distances from a reference database. Script to evaluate prediction Required files for each identity threshold : Training set header ({databasename_IDthreshold}_trainheader.txt) Test set header ({databasename_IDthreshold}.subsets.txt.testIDs.txt) Prediction results ({databasename_IDthreshold_classifier}_predictions.tsv) Move required files in a different directory "],["generating-custom-database-of-the-12s-marker-gene.html", "Generating custom database of the 12S marker gene Evaluate resolution blindspots Notes on resolution blindspot Assessing hybrids", " Generating custom database of the 12S marker gene Sequences from the RDP databases consist of the complete 12S region while the sequences from the barque databases are trimmed to the amplified region of interest. Therefore, I am keeping the sequences from the RDP databases and adding to it some missing sequences. Steps overview Filter RDP database Extract sequence header Keep only sequence header of fish found in Alberta Curate database using the curated sequence names Add missing sequences Download sequences from Genbank Modify sequence header Combine with curated RDP database Optional steps Trim sequences Dereplicate Filter RDP database Extract sequence header grep -e &quot;&gt;&quot; 12S_rdp.fasta &gt; header_12s_rdp.txt Keep only sequence header of fish found in Alberta ### ------------------------- Load the libraries -------------------------------------------------- #### library(dplyr) library(stringr) ### ------------------------- Save path for output -------------------------------------------------- #### path = &quot;/home/kvilleneuve/fish_edna/database&quot; ### ------------------------- Load list of Alberta freshwater fish -------------------------------------------------- #### alberta_fish = read.csv(&quot;/home/kvilleneuve/fish_edna/database/fishbase_alberta_with_missing.csv&quot;, header = TRUE, check.names = FALSE) species = gsub(&quot; &quot;, &quot;_&quot;, alberta_fish$Species) # Replace space with underscore ### ------------------------- Load database sequence header -------------------------------------------------- #### # I am intentionally specifying the wrong separator because I don&#39;t want to separate the columns yet rdp12s_raw = read.table(&quot;/home/kvilleneuve/fish_edna/database/header_rdp_12S.txt&quot;, sep = &quot;,&quot;) filtered = rdp12s_raw %&gt;% filter(if_any(1, str_detect, paste0(species, collapse = &#39;|&#39;))) write.table(filtered, file = paste(path, &quot;/alberta_ID_&quot;, names(list_db)[i], &quot;.txt&quot;, sep =&quot;&quot;), quote = FALSE, row.names = FALSE, col.names = FALSE) Curate database using the curated sequence names Use seqkit tool to filter databases in order to keep only sequences of freshwater fishes found in Alberta using list of ID generated for each database. For RDP sed -i &#39;s/\\t/;/g&#39; 12S_rdp.fasta sed -i &#39;s/\\t/;/g&#39; alberta_ID_RDP_12S.txt seqkit grep -nrf alberta_ID_RDP_12S.txt 12S_rdp.fasta -o 12s_01_alberta_curated_RDP_raw.fasta Add missing sequences Download sequences from Genbank Missing sequences were downloaded from NCBI Modify sequence header Sequence header of downloaded sequences were manually modified to include all ranks. Combine with curated RDP database # 12S cat 12s_01_alberta_curated_RDP_raw.fasta 12s_02_missing_sequences.fasta &gt; 12s_03_alberta_curated_withmissing.fasta # Normalize length seqkit seq -w 60 12s_03_alberta_curated_withmissing.fasta &gt; 12s_04_alberta_curated_clean.fasta Evaluate resolution blindspots To evaluate if the targeted region was 100% identical between certain species, the sequence from the databases were trimmed using the Forward and Reverse primer (rev-comp) with Cutadapt. The sequences were then dereplicated using vsearch derep_fulllength and every sequences removed were evaluated by aligning them with MEGA11. Trim # Forward primer cutadapt -g CCGGTAAAACTCGTGCCAGC -o 12s_alberta_curated_combined_trimF.fasta 12s_04_alberta_curated_clean.fasta # Reverse primer cutadapt -a CAAACTGGGATTAGATACCCCACTATG -o 12s_alberta_curated_combined_trimFR.fasta 12s_alberta_curated_combined_trimF.fasta Dereplicate vsearch --derep_fulllength 12s_alberta_curated_combined_trimFR.fasta --output 12s_alberta_curated_combined_trim_derep.fasta See section Notes on resolution blindspot for more details about which sequences were removed. ::::::::::::::: vsearch Classifier for barque ::::::::::::::::: Copy the curate database to folder 03_database in the barque directory Rename file with shorter name gzip file cp /home/kvilleneuve/fish_edna/database/curated12s_db/12s_04_alberta_curated_clean.fasta /home/kvilleneuve/fish_edna/code/barque/03_databases/12Scurated.fasta gzip 12Scurated.fasta ::::::::::::::: RDP Classifier ::::::::::::::::: Prepare taxonomy file Extract sequence header using grep (2) Replace semi-colon with tab (3) remove &gt; grep -e &quot;&gt;&quot; 12s_04_alberta_curated_clean.fasta &gt; 12s_curated_header.txt sed &#39;s/;/\\t/g&#39; 12s_curated_header.txt &gt; header4rdp.txt sed -i &#39;s/&gt;//g&#39; header4rdp.txt Add header sed -i $&#39;1 i\\\\\\nID\\tcellularOrganisms\\tSuperkingdom\\tKingdom\\tPhylum\\tClass\\tOrder\\tFamily\\tGenus\\tSpecies&#39; header4rdp.txt Prepare sequence file sed &#39;s/;/\\t/g&#39; 12s_04_alberta_curated_clean.fasta &gt; seq4rdp.fasta Train python2 lineage2taxTrain.py header4rdp.txt &gt; ready4train_tax.txt python2 addFullLineage.py header4rdp.txt seq4rdp.fasta &gt; ready4train_seqs.fasta java -Xmx10g -jar ~/rdp_classifier_2.14/dist/classifier.jar train -o 12s_training_files -s ready4train_seqs.fasta -t ready4train_tax.txt Output is a directory specified by the parameter -o which should contain the following files : bergeyTrainingTree.xml genus_wordConditionalProbList.txt logWordPrior.txt wordConditionalProbIndexArr.txt Move into this newly created directory and create the file rRNAClassifier.properties with the following text : # Sample ResourceBundle properties file bergeyTree=bergeyTrainingTree.xml probabilityList=genus_wordConditionalProbList.txt probabilityIndex=wordConditionalProbIndexArr.txt wordPrior=logWordPrior.txt classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.14 Notes on resolution blindspot The following reads from these 9 taxa were removed : NC_035976;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Cypriniformes;Cyprinidae;Carassius;Carassius_auratus_x_Megalobrama_amblycephala_x_Carassius_cuvieri NC_013430;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Cypriniformes;Cyprinidae;undef_Cyprinidae;Carassius_auratus_x_Megalobrama_amblycephala_tetraploid_hybrid NC_006580;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Cypriniformes;Cyprinidae;Carassius;Carassius_auratus NC_012980;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Cypriniformes;Cyprinidae;undef_Cyprinidae;natural_gynogenetic_Carassius_auratus_red_var. NC_014177;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Cypriniformes;Cyprinidae;Carassius;Carassius_gibelio NC068673;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Perciformes;Cottidae;Cottus;Cottus_cognatus NC_037502;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Salvelinus;Salvelinus_malma NC_000860;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Salvelinus;Salvelinus_fontinalis Carassius Sequence : CACCGCGGTTAGACGAGAGGCCCTAGTTGATATTACAACGGCGTAAAGGGTGGTTAAGGATAAATAAAAATAAAGTCAAATGGCCCCTTGGCCGTCATACGCTTCTAGGCGTCCGAAGCCCTAATACGAAAGTAACTTTAATGAACCCACCTGACCCCACGAAAGCTGAGGAA Which was exactly similar between the following taxa : NC 013431 […] Carassius auratus x Megalobrama amblycephala triploid hybrid NC 035976 […] Carassius auratus x Megalobrama amblycephala x Carassius cuvieri NC 013430 […] Carassius auratus x Megalobrama amblycephala tetraploid hybrid NC 012980 […] natural gynogenetic Carassius auratus red var. NC 014177 […] Carassius gibelio Upon researching if Megalobrama amblycephala have been found in Canada and Alberta I found no evidence of such cases. Therefore all the hybrid sequences were removed. A single copy of the trimmed sequence with the following sequence header was instead used : similarseq;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Cypriniformes;Cyprinidae;Carassius;Carassius_auratus_or_gibelio Cottus Sequence : CACCGCGGTTATACGAGAGGCCCAAGTTGACAAACACCGGCGTAAAGCGTGGTTAAGTTAAAAATCGTACTAAAGCCAAACATCTTCAAGACTGTTATACGTAACCGAAGACAGGAAGTTCAACCACGAAAGTCGCTTTATCTGATCTGAATCCACGAAAGCTAAGGAA Which was exactly similar between the following taxa : NC 068673 […] Cottus cognatus NC 028277 […] Cottus bairdii These were all removed from the database and replaced with a single copy of the trimmed sequence with the following sequence header : similarseq;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Perciformes;Cottidae;Cottus;Cottus_bairdii_or_cognatus NC_068673;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Perciformes;Cottidae;Cottus;Cottus_cognatus Salvelinus Sequence : CACCGCGGTTATACGAGAGGCCCTAGTTGATAACTACCGGCGTAAAGAGTGGTTACGGAAAAATGTTTAATAAAGCCGAACACCCCCTCAGCCGTCATACGCACCTGGGGGCACGAAGACCTACTGCGAAAGCAGCTTTAATTGTACCCGAACCCACGACAGCTACGACA Which was exactly similar between the following taxa : NC 000861 […] Salvelinus alpinus NC 037502 […] Salvelinus malma These were all removed from the database and replaced with a single copy of the trimmed sequence with the following sequence header : similarseq;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Salvelinus;Salvelinus_alpinus_or_malma Sequence : CACCGCGGTTATACGAGAGGCCCTAGTTGATAAATACCGGCGTAAAGAGTGGTTACGAAAAAATGTTTAATAAAGCCGAACACCCCCTCAGCCGTCATACGCACCTGGAGGCACGAAGACCTACTGCGAAAGCAGCTTTAATTATACCCGAATCCACGACAGCTACGACA Which was exactly similar between the following taxa : NC 000860 […] Salvelinus fontinalis NC 042195 […] Salvelinus fontinalis x Salvelinus malma Upon researching if the hybrid has been found in Alberta / Canada I found no evidence of the such. The article associated with this genome (https://doi.org/10.1080/23802359.2019.1574636) states that this hybrid was developed in Tonghua, Jilin Province, Republic of China. I therefore removed the hybrid sequence. Salmo Sequence : CACCGCGGTTATACGAGAGACCCTAGTTGATAACTACCGGCGTAAAGAGTGGTTACGGAAAAATATTCAATAAAGCCGAACACCCCCTCAGCCGTCATACGCACCTGGGGGCACGAAGATCTACTGCGAAAGCAGCTTTAATTATGCCTGAACCCACGACAGCTACGACA Which was exactly similar between the following taxa : NC_024032;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Salmo;Salmo_trutta NC_010007;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Salmoniformes;Salmonidae;Salmo;Salmo_trutta For these the complete 12S sequence were similar. Therefore only sequence was kept ([NC_024032]). Assessing hybrids If one of the specie has not been found in Alberta the taxa and it’s sequence were remove. List of remove hybrids : Carassius_auratus_x_Cyprinus_carpio Carassius_auratus_x_Cyprinus_carpio_x_Carassius_cuvieri Cyprinus_carpio_wuyuanensis_x_Carassius_auratus "],["generating-custom-database.html", "Generating custom database", " Generating custom database To build the curated COI database I downloaded the complete COX1 sequences from NCBI. I then validated that our amplified region of interest was included in the downloaded sequences by using cutadapt, this step was especially important for fishes with only partial COX1 sequences. Notes. The reverse primer used below with CutAdapt corresponds to the RevComp of the actual reverse primer sequence (CARAARCTYATRTTRTTYATTCG). /usr/local/bin/cutadapt --action=mask -g TATTTGGYGCYTGRGCCGGRATAG -o trimF.fasta coi_alberta_curated_derep_NCBI.fasta --untrimmed-output readswithnoforward.txt -e 0.2 # Reverse primer /usr/local/bin/cutadapt --action=mask -a CGAATRAAYAAYATRAGYTTYTG -o trimFR.fasta coi_alberta_curated_derep_NCBI.fasta --untrimmed-output readswithnoReverse.txt -e 0.3 === Summary Forward primer === Total reads processed: 74 Reads with adapters: 74 (100.0%) === Summary Reverse primer === Total reads processed: 74 Reads with adapters: 74 (100.0) The following hybrids were removed considering on one of the species is not found in Alberta. Carassius_auratus_x_Cyprinus_carpio Carassius_auratus_x_Cyprinus_carpio_x_Carassius_cuvieri Cyprinus_carpio_wuyuanensis_x_Carassius_auratus All Megalobrama amblycephala ::::::::::::::: vsearch Classifier for barque ::::::::::::::::: Copy the curate database to folder 03_database in the barque directory Rename file with shorter name gzip file cp /home/kvilleneuve/fish_edna/database/curatedcoi_db/coi_alberta_curated.fasta /home/kvilleneuve/fish_edna/code/barque/03_databases/ mv coi_alberta_curated.fasta coicuratedncbi.fasta gzip coicuratedncbi.fasta ::::::::::::::: RDP Classifier ::::::::::::::::: Prepare taxonomy file Extract sequence header using grep (2) Replace semi-colon with tab (3) remove &gt; (4) add header grep -e &quot;&gt;&quot; coi_alberta_curated.fasta &gt; coi_header_4RDP.txt # awk &#39;/^&gt;/{sub(&quot;&gt;&quot;, &quot;&gt;&quot;++i&quot;;&quot;)}1&#39; coi_NCBI_header.txt &gt; coi_header_4RDP.txt #cp coi_NCBI_header.txt coi_header_4RDP.txt sed -i &#39;s/;/\\t/g&#39; coi_header_4RDP.txt sed -i &#39;s/&gt;//g&#39; coi_header_4RDP.txt sed -i $&#39;1 i\\\\\\nID\\tcellularOrganisms\\tSuperkingdom\\tKingdom\\tPhylum\\tClass\\tOrder\\tFamily\\tGenus\\tSpecies&#39; coi_header_4RDP.txt Prepare sequence file The sequence file must be in fasta format and contain a unique identifier without any white space. The accession number makes a good identifier. Anything after the first white space is ignored. The following are acceptable: &gt;DQ248313 ACGATTTTGACCCTTCGGGGTCGATCTCCAACCCTTT &gt;JF735302 k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales CCGAGTTTTCAACTCCCAAACCCCTGTGAACATACCA Replace semi-colon with tab sed &#39;s/;/\\t/g&#39; coi_alberta_curated.fasta &gt; coi_seq_4RDP.fasta #cp coi_alberta_curated_derep_NCBI.fasta coi_seq_4RDP.fasta #awk &#39;/^&gt;/{sub(&quot;&gt;&quot;, &quot;&gt;&quot;++i&quot;\\t&quot;)}1&#39; coi_alb_NCBI_4RDP.fasta &gt; coi_alb_NCBI_4RDP_ID.fasta Train python2 lineage2taxTrain.py coi_header_4RDP.txt &gt; ready4train_taxonomy.txt python2 addFullLineage.py coi_header_4RDP.txt coi_seq_4RDP.fasta &gt; ready4train_seqs.fasta java -Xmx10g -jar ~/rdp_classifier_2.14/dist/classifier.jar train -o COI_training_files -s ready4train_seqs.fasta -t ready4train_taxonomy.txt Output is a directory specified by the parameter -o which should contain the following files : bergeyTrainingTree.xml genus_wordConditionalProbList.txt logWordPrior.txt wordConditionalProbIndexArr.txt Move into this newly created directory and create the file rRNAClassifier.properties with the following text : # Sample ResourceBundle properties file bergeyTree=bergeyTrainingTree.xml probabilityList=genus_wordConditionalProbList.txt probabilityIndex=wordConditionalProbIndexArr.txt wordPrior=logWordPrior.txt classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.14 "],["generating-mock-communities.html", "Generating mock communities", " Generating mock communities Using the article by Morney et al. we identified 16 species we believed to be good candidate to include in the mock communities. Four types of mock communities were designed : 16 species - normalized abundance (richness + / evenness +) 16 species - non-normalized (richness + / evenness -) 8 species - normalized abundance (richness - / evenness +) 8 species - normalized abundance (richness - / evenness -) To generate the low richness communities, 8 species were randomly selected from the 16 species. Required libraries library(dplyr) library(stringr) library(phyloseq) Generate and load required files (list of fishes and databases sequence headers) rich_plus = c(&quot;Salvelinus_fontinalis&quot;,&quot;Oncorhynchus_mykiss&quot;,&quot;Lota_lota&quot;,&quot;Culaea_inconstans&quot;,&quot;Thymallus_arcticus&quot;,&quot;Perca_flavescens&quot;,&quot;Coregonus_clupeaformis&quot;,&quot;Prosopium_williamsoni&quot;,&quot;Salmo_trutta&quot;,&quot;Sander_vitreus&quot;,&quot;Couesius_plumbeus&quot;,&quot;Platygobio_gracilis&quot;,&quot;Coregonus_artedi&quot;,&quot;Catostomus_catostomus&quot;,&quot;Percopsis_omiscomaycus&quot;,&quot;Cottus_cognatus&quot;) # Low richness community # Random subsample to ID which fishes to include in the low richness community rich_low = sample(x=fish_id_bad, size = 8) rich_low #&quot;Salvelinus_fontinalis&quot;,&quot;Oncorhynchus_mykiss&quot;,&quot;Lota_lota&quot;,&quot;Culaea_inconstans&quot;,&quot;Thymallus_arcticus&quot;,&quot;Perca_flavescens&quot;,&quot;Coregonus_clupeaformis&quot;,&quot;Prosopium_williamsoni&quot; :::::::::::::::::::::::::::::::: 12S marker gene :::::::::::::::::::::::::::::::: Extract sequence header name grep -e &quot;&gt;&quot; /home/kvilleneuve/fish_edna/database/curated12s_db/12s_alberta_curated_combined.fasta &gt; /home/kvilleneuve/fish_edna/database/curated12s_db/12s_curated_header.txt Generate read count file for each type of communities Subset sequence header to keep only fish from generated list # Load sequence header from database header12s = read.table(&quot;/home/kvilleneuve/fish_edna/database/curated12s_db/12s_alberta_curated_combined_ready.fasta&quot;) # Set reads per samples (number obtain from the DADA2 workflow) average_reads = 80492 ############################################################ ########## High Richness and high evenness community ########### ############################################################ # Subset sequence header to keep only fish from generate list header12s_richHIGH = header12s %&gt;% filter(if_any(1, str_detect, paste0(rich_plus, collapse = &#39;|&#39;))) # Remove &gt; at the beginning header12s_richHIGH$V1 = gsub(&quot;&gt;&quot;, &quot;&quot;, header12s_richHIGH$V1) # Save list write.table(header12s_richHIGH, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/12s_header_richplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE) # Generate read count file for (richness + / evenness +) mock community. We want the sum of all the ASV in our sample to be similar to the average read count per sample in the regular samples. header12s_richHIGH$count = floor(average_reads/nrow(header12s_richHIGH)) write.table(header12s_richHIGH, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/12s_counts_richplusevenplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;\\t&quot;) ########################################################### ########## Low Richness and high evenness community ########### ########################################################### header12s_richLOW = header12s %&gt;% filter(if_any(1, str_detect, paste0(rich_low, collapse = &#39;|&#39;))) # Remove &gt; at the beginning header12s_richLOW$V1 = gsub(&quot;&gt;&quot;, &quot;&quot;, header12s_richLOW$V1) # Save list write.table(header12s_richLOW, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/12s_header_richlow.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE) # Generate readcount_file for (richness - / evenness +) mock community header12s_richLOW$count = floor(average_reads/nrow(header12s_richLOW)) write.table(header12s_richLOW, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/12s_counts_richlowevenplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;\\t&quot;) ########################################################### ############### Low evenness communities ################# ########################################################### # The low-evenness of both richness types of mock community was manually # generated to have an exponential distribution of reads alternating between # non- and consistently identified fishes. # 12s_counts_richplusevenlow.txt # 12s_counts_richlowevenlow.txt Create FASTA for InSilicoSeq Generate FASTA file with sequences from all 16 fishes used to generate mock community. Because InSilicoSeq only keeps the first 300 nucleotides, the generated amplicon sequence did not initially correspond to our amplified region and no merged reads were generated. I are therefore used cutadapt to remove all the nucleotides before the forward primer while retaining the primer. The same methodology was used for the reverse primer in order to generate a reverse sequence. We then need to seperately generate the forward and reverse reads # Forward primer cutadapt --action=retain -g CCGGTAAAACTCGTGCCAGC -o 12s_seqTRIMF.fasta 12S_allnormline.fasta # Reverse primer cutadapt --action=retain -a CAAACTGGGATTAGATACCCCACTATG -o 12s_seqTRIMR.fasta 12S_allnormline.fasta Use InSilicoSeq Use InSilicoSeq pre-computed error models to generate amplicon reads. Illumina instruments : #!bin/bash ############# High Richness and High Evenness ############# # FORWARD read echo &quot;Generating reads for High Richness and High Evenness&quot; iss generate --genomes 12s_seqTRIMF.fasta --readcount_file 12s_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output 12s-richplusevenplus_L001 --cpus 20 # Delete the reverse (R2) and move R1 to folder fastq rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes 12s_seqTRIMR.fasta --readcount_file 12s_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output 12s-richplusevenplus_L001 --cpus 20 rm *R1* mv *.fastq fastq ############# Low Richness and high evenness ############# echo &quot;Generating reads for Low Richness and High Evenness&quot; # FORWARD read iss generate --genomes 12s_seqTRIMF.fasta --readcount_file 12s_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output 12s-richlowevenplus_L001 --cpus 20 # Delete the reverse (R2) and move R1 to folder fastq rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes 12s_seqTRIMR.fasta --readcount_file 12s_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output 12s-richlowevenplus_L001 --cpus 20 # Delete the forward (R1) and move R2 to folder fastq rm *R1* mv *.fastq fastq ############# High Richness and low evenness ############# echo &quot;Generating reads for High Richness and Low Evenness&quot; # FORWARD read iss generate --genomes 12s_seqTRIMF.fasta --readcount_file 12s_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output 12s-richplusevenlow_L001 --cpus 20 # Delete all the reverse (R2) and move R1 to another folder (fastq) rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes 12s_seqTRIMR.fasta --readcount_file 12s_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output 12s-richplusevenlow_L001 --cpus 20 # Delete all the forward (R1) and move R2 to another folder (fastq) rm *R1* mv *.fastq fastq ############# Low Richness and low evenness ############# echo &quot;Generating reads for Low Richness and Low Evenness&quot; # FORWARD read iss generate --genomes 12s_seqTRIMF.fasta --readcount_file 12s_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output 12s-richlowevenlow_L001 --cpus 20 # Delete all the reverse (R2) and move R1 to another folder (fastq) rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes 12s_seqTRIMR.fasta --readcount_file 12s_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output 12s-richlowevenlow_L001 --cpus 20 # Delete all the forward (R1) and move R2 to another folder (fastq) rm *R1* mv *.fastq fastq rm *.vcf The pattern _001 was then manually added to every fastq generated to match pattern from our other samples. :::::::::::::::::::::::::::::::: COI marker gene :::::::::::::::::::::::::::::::: Extract sequence header name grep -e &quot;&gt;&quot; /home/kvilleneuve/fish_edna/database/curatedcoi_db/coi_alberta_curated_NCBI.fasta &gt; /home/kvilleneuve/fish_edna/database/curatedcoi_db/coi_NCBI_header.txt Generate read count files for each type of communities # Load Sequence header from databases headercoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/curatedcoi_db/coi_NCBI_header.txt&quot;) # Set reads per samples (number obtain from the DADA2 workflow) average_reads = 55424 ############################################################ ########## High Richness and evenness community ########### ############################################################ # High richness community rich_plus = c(fish_id_good, fish_id_bad) # Subset sequence header to keep only fish from generated list headercoi_richHIGH = headercoi %&gt;% filter(if_any(1, str_detect, paste0(rich_plus, collapse = &#39;|&#39;))) headercoi_richHIGH$V1 = gsub(&quot;&gt;&quot;, &quot;&quot;, headercoi_richHIGH$V1) write.table(headercoi_richHIGH, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/coi_header_richplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;;&quot;) # Generate read count file for (richness + / evenness +) mock community. We want the sum of all the ASV in our sample to be similar to the average read count per sample in the regular samples. headercoi_richHIGH$count = floor(average_reads/nrow(headercoi_richHIGH)) write.table(headercoi_richHIGH, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/coi_counts_richplusevenplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;\\t&quot;) ########################################################### ########## Low Richness and evenness community ########### ########################################################### # Low richness community rich_low = c(&quot;Oncorhynchus_mykiss_x_Salmo_salar&quot;,&quot;Notropis_atherinoides&quot;,&quot;Percina_caprodes&quot;,&quot;Coregonus_artedi&quot;,&quot;Sander_vitreus&quot;,&quot;Percopsis_omiscomaycus&quot;,&quot;Culaea_inconstans&quot;,&quot;Lota_lota&quot;) # Subset sequence header to keep only fish from generated list headercoi_richLOW = headercoi %&gt;% filter(if_any(1, str_detect, paste0(rich_low, collapse = &#39;|&#39;))) headercoi_richLOW$V1 = gsub(&quot;&gt;&quot;, &quot;&quot;, headercoi_richLOW$V1) write.table(headercoi_richLOW, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/coi_header_richlow.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE) # Generate read count file for (richness - / evenness +) mock community. headercoi_richLOW$count = floor(average_reads/nrow(headercoi_richLOW)) write.table(headercoi_richLOW, &quot;/home/kvilleneuve/fish_edna/raw/mock_comm/coi_counts_richlowevenplus.txt&quot;, quote = FALSE, col.names = FALSE, row.names = FALSE, sep = &quot;\\t&quot;) ########################################################### ############### Low evenness communities ################# ########################################################### # The low-evenness of both richness types of mock community was manually # generated to have an exponential distribution of reads alternating between # non- and consistently identified fishes. Create FASTA for InSilicoSeq Generate FASTA file with sequences from all 16 fishes used to generate mock community and make this fasta linear seqkit grep -nrf coi_header_richplus.txt Because InSilicoSeq only keeps the first 300 nucleotides, the generated amplicon sequence did not initially correspond to our amplified region and no merged reads were generated. I therefore used cutadapt to remove all the nucleotides before the forward primer while retaining the primer. For the reverse primer, the actual primer sequence was not consistently present in our sequences. Increasing the -e value did allow the primer to be found but this resulted in sequences less then 300 bp for some fishes. I therefore decided to instead cut the trimF fasta at 300 bp. cutadapt --action=retain -g GTATTTGGYGCYTGRGCCGGRATAGT -o coi_seq_TRIMF.fasta coi_all.fasta -e 0.2 cutadapt --action=retain -a CGAATRAAYAAYATRAGYTTYTG -o coi_seq_TRIMR.fasta coi_all.fasta -e 0.2 #cutadapt --length 350 -o coi_seq_TRIMFR.fasta coi_seq_TRIMF.fasta InSilicoSeq Use InSilicoSeq pre-computed error models to generate amplicon reads. # High Richness and high evenness iss generate --genomes coi_seq_TRIMFR.fasta --readcount_file coi_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenplus_L001 # High Richness and low evenness iss generate --genomes coi_seq_TRIMFR.fasta --readcount_file coi_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenlow_L001 # Low Richness and high evenness iss generate --genomes coi_seq_TRIMFR.fasta --readcount_file coi_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenplus_L001 # Low Richness and low evenness iss generate --genomes coi_seq_TRIMFR.fasta --readcount_file coi_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenlow_L001 The pattern _001 was then manually added to every FASTQ generated to match pattern from our other samples. #!/bin/bash ############# High Richness and High Evenness ############# # FORWARD read echo &quot;Generating reads for High Richness and High Evenness&quot; iss generate --genomes coi_seq_TRIMF.fasta --readcount_file coi_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenplus_L001 --cpus 20 # Delete the reverse (R2) and move R1 to folder fastq rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes coi_seq_TRIMR.fasta --readcount_file coi_counts_richplusevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenplus_L001 --cpus 20 rm *R1* mv *.fastq fastq ############# Low Richness and high evenness ############# echo &quot;Generating reads for Low Richness and High Evenness&quot; # FORWARD read iss generate --genomes coi_seq_TRIMF.fasta --readcount_file coi_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenplus_L001 --cpus 20 # Delete the reverse (R2) and move R1 to folder fastq rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes coi_seq_TRIMR.fasta --readcount_file coi_counts_richlowevenplus.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenplus_L001 --cpus 20 # Delete the forward (R1) and move R2 to folder fastq rm *R1* mv *.fastq fastq ############# High Richness and low evenness ############# echo &quot;Generating reads for High Richness and Low Evenness&quot; # FORWARD read iss generate --genomes coi_seq_TRIMF.fasta --readcount_file coi_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenlow_L001 --cpus 20 # Delete all the reverse (R2) and move R1 to another folder (fastq) rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes coi_seq_TRIMR.fasta --readcount_file coi_counts_richplusevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richplusevenlow_L001 --cpus 20 # Delete all the forward (R1) and move R2 to another folder (fastq) rm *R1* mv *.fastq fastq ############# Low Richness and low evenness ############# echo &quot;Generating reads for Low Richness and Low Evenness&quot; # FORWARD read iss generate --genomes coi_seq_TRIMF.fasta --readcount_file coi_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenlow_L001 --cpus 20 # Delete all the reverse (R2) and move R1 to another folder (fastq) rm *R2* mv *.fastq fastq # REVERSE read iss generate --genomes coi_seq_TRIMR.fasta --readcount_file coi_counts_richlowevenlow.txt --sequence_type amplicon --model MiSeq --output coi-richlowevenlow_L001 --cpus 20 # Delete all the forward (R1) and move R2 to another folder (fastq) rm *R1* mv *.fastq fastq rm *.vcf for f in *.fastq; do mv “\\(f&quot; &quot;\\)(echo”$f” | sed s/IMG/VACATION/)“; done "],["supplementary-workflow.html", "Supplementary workflow Curating databases for distance matrix", " Supplementary workflow Curating databases for distance matrix Required files : barque COI 3 (barque_coi.fasta) RDP COI V5.1.0 5 (rdp_coi.fasta) RDP Filter out sequence which do not belong to class Actinopteri seqkit grep -r -n -p &#39;.*Actinopteri*&#39; coi_rdp.fasta -o coi_rdp_acti.fasta Extract sequences header and print into txt file grep &quot;&gt;&quot; acti_rdp_coi.fasta acti_rdp_coi.fasta The file with the sequence headers can then be passed to R to generate a list of headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint. #### ------------------------- Load libraries ------------------------------------------------------------------#### library(dplyr) library(tidyr) #### ------------------------- Load databases sequence header -------------------------------------------------- #### rdpcoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt&quot;,sep = &quot;;&quot;) #### ------------------------- Format dataframes -------------------------------------------------------------- #### names(rdpcoi) = c(&quot;ID&quot;, &quot;Cellular_organism&quot;,&quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) rdp_coi_actino = subset(rdpcoi, Class == &quot;Actinopteri&quot;) #### ------------------------- Random sampling --------------------------------------------------------------- #### random_sampling_list = list() i = 0 for (genus in unique(rdp_coi_actino$Genus)){ i = i + 1 sub_df = subset(rdp_coi_actino, Genus == genus) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } random_sampling_list[[i]] = rand_sub_df } #### ------------------------- Generate list of headers to keep ----------------------------------------------- #### df = do.call(&quot;rbind&quot;, random_sampling_list) df = df %&gt;% unite(&quot;sequence_header&quot;, ID:Species, remove = FALSE, sep = &quot;;&quot;) write.table(df$sequence_header, &quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_headers_to_keep.txt&quot;, col.names = FALSE, row.names = FALSE, quote = FALSE) Pass the file generated by the R script to seqkit to create a subset database seqkit grep -nrf rdp_coi_headers_to_keep.txt chord_rdp_coi.fasta -o curated_rdp_coi.fasta Barque Remove sequence of taxa which do not belong to phylum Chordata seqkit grep -r -n -p &#39;.*chordata_*&#39; barque_coi.fasta -o chord_barque_coi.fasta Add unique ID to every sequence header, this is required to randomly sample 10 sequences per genus awk &#39;/^&gt;/{sub(&quot;&gt;&quot;, &quot;&gt;&quot;++i&quot;_&quot;)}1&#39; chord_barque_coi.fasta &gt; chord_barque_coi_ID.fasta Extract sequences header and print into txt file grep &quot;&gt;&quot; chord_barque_coi_ID.fasta | sed -e &quot;s/&gt;//&quot; &gt; barque_coi_ID_header.txt grep &quot;&gt;&quot; rdp_coi.fasta | sed -e &quot;s/&gt;//&quot; &gt; rdp_coi_header.txt The file with the sequence headers can then be passed to R to generate a list of which headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint. #### ------------------------- Load libraries ------------------------------------------------------------------#### library(dplyr) #### ------------------------- Load databases sequence header -------------------------------------------------- #### barquecoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_ID_header.txt&quot;, sep = &quot;_&quot;) rdpcoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt&quot;,sep = &quot;;&quot;) #### ------------------------- Format dataframes for merging --------------------------------------------------- #### # The sequence headers of the COI database used by barque has the following format : Phylum_Genus_Species # Therefore, to keep only sequence belonging to class Actinopteri # I am merging the header from the barque COI database with headers from the RDP COI database to get complete # taxonomic rank barquecoi$Species = paste(barquecoi$V2, barquecoi$V3, sep = &quot;_&quot;) # Create column with specie name barquecoi_unique = barquecoi[!duplicated(barquecoi), ] # Keeping only unique value # Add column names to RDP database names(rdpcoi) = c(&quot;ID&quot;, &quot;Cellular_organism&quot;,&quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) rdpcoi_noID = subset(rdpcoi, select = -c(ID)) # Remove column ID (they are unique values) rdpcoi_unique = rdpcoi_noID[!duplicated(rdpcoi_noID), ] # Keeping only unique value #### ------------------------- Merge dataframes -------------------------------------------------------------- #### comb_df = merge(barquecoi_unique, rdpcoi_unique, by = &quot;Species&quot;, all.y = FALSE ) #### ------------------------- Random sampling --------------------------------------------------------------- #### random_sampling_list = list() i = 0 for (genus in unique(comb_df$Genus.x)){ i = i + 1 sub_df = subset(comb_df, Genus.x == genus) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } random_sampling_list[[i]] = rand_sub_df } df = do.call(&quot;rbind&quot;, random_sampling_list) #### ------------------------- Generate list of headers to keep ----------------------------------------------- #### df$barque_coi_header = paste(df$ID, df$Phylum.x, df$Species, sep = &quot;_&quot;) write.table(df$barque_coi_header, &quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_headers_to_keep.txt&quot;,col.names = FALSE, row.names = FALSE, quote = FALSE) Pass the file generated by the R script to seqkit to create a subset database. seqkit grep -nrf barque_coi_headers_to_keep.txt chord_barque_coi.fasta -o curated_barque_coi.fasta "],["random-subsample-10-sequences.html", "Random subsample 10 sequences", " Random subsample 10 sequences We used the curated sequences from the GSL-rl to evaluate the accuracy and precision of species assignments using NCBI-nt (Fig. 1B). Taxonomic assignments were performed over the sequences contained in GSL-rl using NCBI-nt (downloaded 2020-10-23) and the BLAST+ tool blastn (v2.10.1, Camacho et al. 2009) combined with the least common ancestor (LCA; hereafter BLAST-LCA) or the Top Hit methods (hereafter, BLAST-TopHit) at three identity thresholds (95, 97 and 99%) with an in-house R script. The database we are evaluating : 0.0.0.0.0.0.0.0.0.0.0.0.0.0.1 —— 12S —— 12S.fasta =&gt; 12s_vsearch.fasta —— Evaluate with vsearch - Evaluate with vsearch 12S_rdp.fasta =&gt; 12s_rdp.fasta —— Evaluate with RDP —– Evaluate with RDP 12s_alberta_curated_combined_ready.fasta =&gt; 12s_cur.fasta — Evaluate with RDP and vsearch 0.0.0.0.0.0.0.0.0.0.0.0.0.0.2 —— COI —— bold.fasta =&gt; coi_vsearch.fasta —————————– Evaluate with vsearch mytrainseq.fasta =&gt; coi_rdp.fasta —————————- Evaluate with RDP coi_alberta_curated_derep_NCBI.fasta =&gt; coi_cur.fasta ——– Evaluate with RDP and vsearch For each COI database I used an R script to randomly select 10 species RDP : raw_rdp = read.table(&quot;/home/kvilleneuve/fish_edna/code/evalute_db/coi_rdp_header.txt&quot;, sep = &quot;;&quot;) rdp_10sp_list = list() i = 0 for (species in unique(raw_rdp$V10)){ i = i + 1 sub_df = subset(raw_rdp, V10 == species) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } rdp_10sp_list[[i]] = rand_sub_df } rdP10sp = do.call(&quot;rbind&quot;, rdp_10sp_list) write.table(rdP10sp, &quot;/home/kvilleneuve/fish_edna/code/evalute_db/coi_rdp_header2keep.txt&quot;, quote = FALSE, row.names = FALSE, col.names = FALSE, sep = &quot;;&quot;) sed -i &#39;s/&gt;//g&#39; coi_rdp_header2keep.txt seqkit grep -f coi_rdp_header2keep.txt coi_rdp.fasta -o coi_rdp_10sp.fasta --threads 20 VSEARCH : raw = read.table(&quot;/home/karine/coi_vsearch_header.txt&quot;, sep = &quot;_&quot;) raw$Species = paste(raw$V2, raw$V3, sep = &quot;_&quot;) list_10sp = list() i = 0 for (specie in unique(raw$Species)){ i = i + 1 sub_df = subset(raw, Species == specie) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } list_10sp[[i]] = rand_sub_df } final_10sp = do.call(&quot;rbind&quot;, list_10sp) final_10sp$good_name = paste(final_10sp$V1, final_10sp$V4, sep = &quot;_&quot;) final_10sp$good_name = gsub(&quot;&gt;&quot;, &quot;&quot;, final_10sp$good_name) final = as.data.frame(final_10sp$good_name) write.table(final, &quot;/home/karine/coi_vsearch_header2keep.txt&quot;, quote = FALSE, row.names = FALSE, col.names = FALSE, sep = &quot;;&quot;) ./seqkit grep -f coi_vsearch_header2keep.txt coi_vsearch.fasta -o coi_vsearch_10sp.fasta --threads 40 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
